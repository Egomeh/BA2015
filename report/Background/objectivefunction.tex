\subsection{Objective function}

The optimizing algorithms used in this thesis both attempts to 
optimize a certain function. In this case, the optimization 
function aims to develop the best possible Tetris playing agent.
Thus, the value of the objective function describes an estimated 
performance of the input agent. The objective function serves as
an abstraction to the Tetris emulator that will evaluate 
the agent by letting the agent play a pre-configured number 
of games. The objective function that estimates the performance 
of an agent is later referred to as $\fitnessFunction$.\\
\\
When the Tetris simulator plays Tetris, the internal decision process
of the Tetris controller is configured with a set of parameters which remain
fixed across a single game. These parameters are the weights associated 
with each feature function that is considered by the controller. 
The features $\feature _{i}$
each map a state $\gameState$ of the game to a real value. 
An overview of the exact mappings
can be seen in table 1 in \citep{scherrer2009:b}. How much each of these mappings
should affect the final evaluation of the board is determined by $\weight _{i}$
denoting the weight of the $i$-th feature.
Finally, the function to assess the value 
of current board state $\valueFunction \left( \gameState \right)$, 
with $\dimensions$ features functions present, can be expressed as:
\begin{align*}
\valueFunction (\gameState ) &= 
\sum_{i=1}^{\dimensions} \weight _{i}\feature _{i}(\gameState )
\end{align*}
Let $\allGameStates$ be the set of all states that each possible 
action can lead to from the current state $\gameState$. The 
controller will then for each reachable state 
$gameState_i \in \allGameStates$ evaluate the state by 
$\fitnessFunction \left( \gameState_i \right)$. 
The chosen action is the one that yields the state of the highest value.
The performance of the controller is hence directly tied to the 
features and weights in the evaluation function. To adjust these controllers,
one can either change the set of applied feature functions, or as the 
optimization algorithms will do, change the weighting of the features.
For the experiments, the feature sets remain fixed, and the task of the
optimization algorithm applied is to tune the set of weights in order 
to maximize the performance of the controller.\\
\\
In summary, the objective function accepts a vector of values for each weight
and configures an agent with the weights corresponding to the entries in
the input vector. It then plays $\numberOfEvaluations$ games with the
agent and reports the mean score.

\tmpcom{
% evaluating a specific agent
Now that the new generation has been created, 
it's time for each agent to play g games of Tetris.\\
For a given Tetris game, we say that a game consists 
of a range of states. A state is defined as, when an 
agent has to place a given piece, and it has to choose 
where to place the piece on the board. Placing the 
piece is called an action, and since there are multiple 
ways to place the piece on the board, we have a range 
of possible of actions. In a given state the agent 
has to try each possible action, and the action 
with the highest evaluation, is the action the 
agent executes in the current state. 
When evaluating each possible action, the 
objective function S is used. As shown in the 
input-section, the objective function is 
defined as:
\begin{align*}
\valueFunction (\gameState ) &= \sum_{i=1}^{\dimensions} \weight _{i}\feature _{i}(\gameState )
\end{align*}

Where $F_i$ is the i'th feature function 
and w\_i is the i'th weight associated 
with the corresponding i'th feature function. 
More specifically w\_i is the value of 
the i'th dimension of the agent's vector.
A specific action then gets evaluated 
by computing S(x). The main factor of 
the evaluation value is how the action 
changed the board layout , since each 
feature function F\_i (x) looks at different 
"types" of board layouts (insert ref. to feature policy table).
}


