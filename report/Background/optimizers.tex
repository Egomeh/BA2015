\subsection{Optimizers \label{Optimizers}}

As mentioned earlier, both the Cross-entropy method and CMA-ES fall into the category of 
\textit{stochastic optimization}
methods. These methods are useful for 
optimization problems where the gradient is not available.
The optimization functions aim to optimize 
the parameter set $\textbf{\individual }$
for the objective function $\fitnessFunction$.
\begin{align}
\hat{\textbf{\individual }} &= 
arg \  \underset{\textbf{\individual }}{max} \  
\fitnessFunction (\textbf{\individual }) \ 
:\mathbb{R}^{\dimensions } \rightarrow \mathbb{R}
\end{align}

Often, the problem is depicted as a minimization problem which is quite
the opposite in relation to Tetris where the goal is to gain as high score as possible.
Therefore we depict the problem entirely as a optimization problem.
In these optimization methods, the optimizing algorithm uses a family of parametric distributions,
and maintains a set of parameters for the used distribution
to search the best possible solution for the objective function.  
In the case studied in this thesis
both, the CMA-ES and Cross-entropy method use a 
Gaussian distribution to sample solutions to the objective function.
Hence, both functions aim to find a mean 
$\mean $ and an $\dimensions \times \dimensions$ matrix 
$\varianceMatrix $\footnote{In \citep{hansen2011}, 
$\sigma$ is used for step-size in CMA-ES, so $\varianceMatrix $ is instead introduced
as an arbitrary $\dimensions \times \dimensions$ matrix in its place.}, such that when
a vector $\textbf{\individual }$ is sampled by 
$\textbf{\individual } \sim \mathcal{N}\left( \mean, \varianceMatrix \right)$, 
then $\fitnessFunction (\textbf{\individual })$ 
is likely to yield the desired results.\\
\\
The algorithms work iteratively, such that the mean and variance 
of the distribution 
is altered for each iteration $\generation $.
The algorithms start by initializing the 
parameters either at random or at some fixed point. A common 
configuration is setting the mean to 
a zero-vector and the standard deviation to the identity matrix.
Thus, for the first iteration $\generation = 0$, a configuration could be as follows
\begin{align}
\mean^{(0)} =
\begin{bmatrix}
0\\
\vdots\\
0
\end{bmatrix},\ \ 
\varianceMatrix^{(0)} = 
\begin{bmatrix}
1 & \hdots & 0\\
\vdots & \ddots & \vdots\\
0 & \hdots & 1
\end{bmatrix}
\end{align}

The superscript of $(0)$ notes that the values occur in generation 0.\\
\\
In each generation, the algorithms sample $\populationSize$ search points
and evaluate their fitness
against the objective function. When each of the solutions are evaluated,
they are ordered according to their fitness
\begin{align}
\{\textbf{\individual }_{1}, \hdots, 
\textbf{\individual }_{\populationSize }\}\ \ \text{Such that}\ \ 
\fitnessFunction(\textbf{x}_1) \geq 
\fitnessFunction(\textbf{\individual }_2), \hdots, 
\fitnessFunction(\textbf{\individual }_{\populationSize  - 1}) \geq 
\fitnessFunction(\textbf{\individual }_{\populationSize })
\end{align}
This sorting is a rather essential part of how the search parameters are
updated. Both the Cross-entropy method and CMA-ES algorithm use a fraction of the 
best solutions, that were drawn from its Gaussian distribution. An essential 
part of the selection is that the candidate solutions are not contributing
directly according to their yield from the \textit{fitness function}, but rather
how they are ranked relative to the other solutions.
The mean and standard deviation for the next iteration,
$m^{(\generation+1)}$ and $M^{(\generation+1)}$, are
then updated usually by considering the best of the ordered solutions. How exactly
these parameters are updated is individual for each method is elaborated further in the
following sections.


