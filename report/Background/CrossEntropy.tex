\subsection{CE (Cross Entropy) \label{CrossEntropy}}
CE is described through many papers in 
slightly different ways. This section
describes the algorithm in a similar fashion 
to \citep{thiery:09}.\\
\\
This method uses a Gaussian distribution to 
sample sample vectors for the objective function
$\fitnessFunction$. The algorithm aims to 
adjust the parameters of the distribution
such that samples $\individual$ drawn from the distribution
gives the best possible results when applied to the
objective function $\fitnessFunction \left( \individual \right)$.\\
\\
The Cross-Entropy method starts with an initial 
mean $\mean$ which is an $n$-dimensional vector
typically set to 0
\begin{align}
\mean^{(0)} = \begin{pmatrix}
0\\
\vdots\\
0
\end{pmatrix} 
\end{align}

The second argument to the gaussian distribution is a 
diaginal matrix which contains the variance for each 
component

\begin{align}
\varianceMatrix^{(0)} =
\begin{bmatrix}
\sigma_1^{2} & \hdots & 0\\
\vdots & \ddots & \vdots\\
0 & \hdots & \sigma_{\dimensions}^{2}
\end{bmatrix}
\end{align}
The algorithm will in each generation sample a set of search points
and rank these using the objective function.
In each generation, $\populationSize$ vectors are sampled by 
$\individual_{i} \sim \mathcal{N} \left(\mean, \varianceMatrix \right)
,\ i \in \{1,\dots,\populationSize \}$. The vectors are all evaluated 
against the fitness function and ordered such that $\fitnessFunction \left( \individual_{1} \right) \geq, \dots, \geq \fitnessFunction \left( \individual_{\populationSize} \right)$
, and the $\offspringNumber$ best are chosen for updating the distribution 
parameters. The mean is updated as the centroid of the chosen vectors, and
the variance in each dimension is set to the variance of the chosen vectors in each 
dimension.\\

The pseudo code and details of the algorithm can be seen in figure
\ref{fig:ceCode} on page \pageref{fig:ceCode}.

\begin{figure}[H]
\hrule
\vspace{0.2cm}
{\centering  \textit{Noisy cross-entropy method}}
\vspace{0.2cm}
\hrule
\begin{algorithmic}
\State{\textbf{input}}
\State{$\fitnessFunction$ : The function that estimates the performance of a vector $\individual$}
\State{($\mean^{(0)}$, $\varianceMatrix^{(0)}$): The mean and variance of the initial distribution}
\State{$\populationSize$ : The number of vectors sampled per generation/iteration}
\State{$\offspringNumber$: The number of offspring selected for the new mean}
\State{$\noise_{\generation}$: The noise added to each generation/iteration}
\\

\Loop
\State{Generate $\populationSize$ vectors $\individual_{1}, \individual_{2}, \dots, \individual_{\populationSize}$ from $\mathcal{N}(\mean^{(\generation)}, \varianceMatrix^{(\generation)})$}
\State{Evaluate each vector using $\fitnessFunction$}
\State{Select the $\offspringNumber$ vectors with the highest evaluation}
\State{Update $\mean^{(\generation + 1)}$ based on the $\offspringNumber$ best vectors}
\State{Update $\varianceMatrix^{(\generation + 1)}$ based on the $\offspringNumber$ best vectors + $\noise^{(\generation)}$}
\EndLoop
\end{algorithmic}
\hrule
\caption{The pseudo code for the Cross-Entropy algorithm \label{fig:ceCode}}
\end{figure}

\subsubsection{Input}

\textbf{The objective function \label{CEObjective}} \\
The objective function $\fitnessFunction$ is a 
function used to rank the value of a sampled vector.
As described in the 'Optimizers' section, CE is a general stochastic 
iterative algorithm that tries to solve an optimization problem of 
the form \citep{thiery:09}:
\begin{align}
\hat{\textbf{\individual }} &= 
arg \  \underset{\textbf{\individual }}{max} \  
\fitnessFunction (\textbf{\individual }) \ 
\end{align}
Where $x$ corresponds to a given vector, 
and $\fitnessFunction$ is our actual objective function. 
\\

\textbf{The mean and variance of the gaussian distribution} \\
Here $\mean^{(\generation)}$ is the mean and  
$\varianceMatrix^{(\generation)}$ is the covariance matrix, with 
the variance in each dimension along the diagonal, and zeros elsewhere. 
The parameters in the Gaussian distribution are ($\mean^{(\generation)}$,
$\varianceMatrix^{(\generation)}$). 
More specifically this Gaussian distribution is defined as 
\begin{align}
\mathcal{N}(\mean^{(\generation)},\varianceMatrix^{(\generation)})
\end{align}

Where $\generation$ denotes the current iteration.\\


\textbf{The number of vectors}\\
$\populationSize$ is the number of vectors sampled in each generation.
\\

\textbf{The number of selected vectors / parents}\\
$\offspringNumber$ is the number of vectors which are selected to compute 
the new mean $\mean^{(\generation + 1)}$, and variance
$\varianceMatrix^{(\generation + 1)}$, for next generation/iteration. 
These offspring vectors gets selected 
directly by taking the $\offspringNumber$ best ranked
vectors.
\\

\textbf{The noise term}\\
The noise factor, $\noise^{(\generation)}$, 
is the amount of noise which 
is applied to the variance $\varianceMatrix^{(\generation)}$ in
iteration/generation 
$\generation$. In general, noise is used to prevent
the algorithm from 
getting narrowing down to an undesired local optimum, but
rather explore new solutions.
The noise term can be any
function of $\generation$. Among 
the most described noise types are: no noise, constant noise 
and linear decreasing noise \citep{szita:06}. 
When using no noise, $\noise^{(\generation)}$ 
is simply set to zero. When using constant noise, 
the same value is 
added to the variance $\varianceMatrix^{(\generation)}$ 
in each iteration/generation. 
When using linear decreasing noise, 
$\noise^{(\generation)}$ is often set as
$\noise^{(\generation)}=max(5- \generation /10,0)$.
\\

\subsubsection{Loop}

\textbf{Sampling the population}\\
The first step of the loop is to generate the new generation 
consisting of $\populationSize$ vectors. 
These vectors are drawn from the distribution 
$\individual_{i}\sim \mathcal{N}(\mean^{(\generation)},\varianceMatrix^{(\generation)})$.
\\

\textbf{Evaluating the population}\\
After sampling the population, the algorithm needs to order the vectors to find the $\offspringNumber$ best vectors, each vector $\individual_{i}\ ,\ i \in \{1, \dots, \populationSize\}$ is evaluated using $\fitnessFunction$. 
The value from the objective function then yields 
the estimated performance of each individual.
\\

\textbf{Selecting the parents}\\
As each $\individual_{i}$ has an assigned evaluation value, and the 
$\offspringNumber$ best vectors gets selected by 
taking the $\individual_{i}$ vectors with the highest ranking.
\\

\textbf{Updating the distribution parameters}\\
When updating the distribution parameters for the next iteration
($\mean^{(\generation + 1)}$,$\varianceMatrix^{(\generation + 1)}$), 
the mean is updated by computing the centroid of the 
$\offspringNumber$ highest ranking vectors. This is formally defined as:
\begin{align}
\mean^{(\generation +1)}:=\frac{\sum_{i=1}^{\offspringNumber} \individual_{i}}{\offspringNumber}
\ \ \ 
\text{where}
\ \ \ 
\fitnessFunction \left( \individual_{1} \right) \geq, \dots, 
\geq \fitnessFunction \left( \individual_{\offspringNumber} \right) \geq, 
\dots, \geq \fitnessFunction \left( \individual_{\populationSize} \right)
\end{align}
The diagonal matrix of variances $\varianceMatrix^{(\generation + 1)}$ is updated 
to match the variance of the $\offspringNumber$ best
vectors, such that the variance in dimension $k$ 
matches the variance of the $\offspringNumber$
in dimension $k$. Hence, for $k \in \{ 1,\dots,\dimensions \}$,
the $(k,k)$'th entry in the variance matrix is 
updated 
This is formally defined as:
\begin{align}
{\varianceMatrix^{(\generation +1)}}_{k,k}:=\frac{\sum_{i}^{\offspringNumber}
({\individual_{i}}_k - \mean_{\generation +1})^2}{\offspringNumber} + 
\noise_{\generation + 1}\ ,\ \ k \in \{ 1,\dots,\dimensions \}
\end{align}