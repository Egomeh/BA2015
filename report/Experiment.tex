\section{Experiment settings}

This section will walk through our experimental setup, our verification of CE and 
and a analysis/discussion of the results.

\subsection{MDP-Tetris}

When running the experiments, the source code of the MDP-Tetris
\citep{mdptetris} was used to emulate the Tetris games.
The source code is accompanied with files that describe the
various existing features. These files contains the identifiers of 
each feature to use, as well as two numbers respectively describing 
the agents reward function and how to evaluate a game over state. 
The number for the reward function has remained unchanged at $0$ 
during all experiments. The "game-over" evaluation was for the
Bertsekas feature set initially set to $0$. Setting the 
"game-over" evaluation to $0$ means that the agent will not 
distinguish between regular moves and moves that results in losing
the game. When running the experiments with this setting, a large portion
of the agents never exceeded a zero mean score. However, setting the value
to $-1$, meaning that a "game-over" move yields $-\infty$ reward, 
none of the experiments got stuck on only zero scores. An example
of the layout of the feature file can be seen in figure \ref{fig:featfile}.
\begin{figure}[h!]
\centering
\begin{lstlisting}
0    <- Describes the reward function
-1   <- Actions leading to game over is avoided at all cost
22   <- The policy contains 22 features
8 0  <- The feature with id 8 initially has weight 0
...  <- The remaining 21 features
\end{lstlisting}
\caption{Example of a file that describes a feature set. \label{fig:featfile}}
\end{figure}

 
\subsection{Normalization of samples \label{normalSamples}}
As mentioned by some authors \citep{boumaza2009}, the vector that
describes the agent can very well be normalized such that the vector
is a point that lies on the $\dimensions$-dimensional hypersphere.\\
\\
The reason for this lies in the nature of the evaluation function.
When the controller chooses an action, it will evaluate all the 
possible actions possible with the current piece. It will use the 
value function $\valueFunction$ of each state $\gameState_i$ and 
choose the state with the highest value from the value function.
Thus, if the states are ordered such that:
\begin{align*}
\valueFunction \left(  \gameState_1 \right) 
> \dots 
> \valueFunction \left( \gameState_\populationSize \right)
\end{align*}

The agent then chooses the action that transitions from the current state 
to state $\gameState_1$.\\
Since the value function assess the state by the following:
\begin{align*}
\valueFunction (\gameState ) &= 
\sum_{i=1}^{\dimensions} \weight _{i}\feature _{i}(\gameState )
\end{align*}

Then scaling the input of the agent, the weight vector $\weight$, by a
number $a \in \mathbb{R}, \ a > 0$ the assessment is changed by:
\begin{align*}
\sum_{i=1}^{\dimensions} a \weight _{i}\feature _{i}(\gameState ) &= 
a\sum_{i=1}^{\dimensions} \weight _{i}\feature _{i}(\gameState )\\
&= a \valueFunction \left( \gameState \right)
\end{align*}

And the ordering remains:
\begin{align*}
a \valueFunction \left(  \gameState_1 \right) 
> \dots 
> a \valueFunction \left( \gameState_\populationSize \right)
\end{align*}

Thus the order of the value functions of 
each state does not change, and the same $\gameState_1$
is still chosen for any $a \in \mathbb{R}, \ a > 0$.\\
\\
To verify this, the Tetris objective function was executed with the
same vector and the same seed for the random generator with a scale
$a \in \{0.1, 0.2,0.3, \dots, 9.8,9.9,10.0\}$, and the agent scored 
exactly the same for each scale.\\
\\
This can be used in experiments for various reasons. As reported 
in \citep{boumaza2009}, normalizing the samples will 
prevent CMA-ES from diverging in step size,
and it can prevent loosing precision if the magnitude of weight 
vector becomes larger than feasible for the used floating 
point number and avoids size limitations.



\subsection{Setup}

When executing the experiments, various parameters each have 
an impact on the final result of the learning curve. Thus, the parameters
are adjusted, first to match the experiments run by other researchers, 
and later to conduct as fair as possible comparisons between 
Cross-Entropy and CMA-ES.\\
\\
% agents
The amount of vectors sampled in each generation $\populationSize$
has a high impact on the algorithm performance. By setting $\populationSize$
high, more policies are evaluated per iteration, and leads to a more thorough 
exploration of the search space. Thus the higher $\populationSize$ increases the
chances of finding a better mean for the next iteration.
However, higher $\populationSize$ also results in the
need for more evaluations per iteration. The goal for 
tuning this parameter is then
to set $\populationSize$ high enough to ensure 
exploration of good solutions, and yet 
low enough to avoid unnecessary evaluations.\\
In the implementation of CMA-ES from \shark , 
the algorithm  itself determines
the value of $\populationSize$ according to the 
size of the search space. 
Cross-Entropy however, does not seem to have a 
general rule for this parameter,
so this value is manually adjusted to fit the 
problem as well as possible.\\
\\
% offspring
As both of the optimizing algorithm uses a subset of the sampled vectors
from a generation to update the distribution parameters, the number of 
offspring $\offspringNumber$ influences how the next generation is sampled.
By setting the value too high, the algorithm risks ceasing to progress any 
further since the updated mean would be too close to the previous one to 
significantly make a difference. By setting the value too low,
the risk of reaching a local optimum increases since the high-scoring agents
might have reached their high performance by chance.\\
The CMA-ES itself manages setting $\offspringNumber$ and Cross-Entropy
is set according to the problem. Most authors that uses Cross-Entropy for Tetris
sets the offspring size to $10\%$ of population size, that is 
$\offspringNumber = \lfloor 0.1 \cdot \populationSize \rfloor $.\\
\\
% Number of games per iteration
The number of games, $\numberOfEvaluations$, 
is the number of games  which each agent 
plays in each iteration. An agent's score is defined as the 
mean of the score of these 
$\numberOfEvaluations$ games.
We want this value low as possible, because as with the number of
agents, $\populationSize$, The number of games, $\numberOfEvaluations$, 
is another major factor in the run-time of the algorithm.
As Tetris is stochastic by nature, the score deviates a lot, 
even when the
same agent with the same policy plays multiple games. 
Hence, when assessing the true
performance of a policy it's rarely enough to play just few games. Thus, setting 
$\numberOfEvaluations$ high increases the likelihood of correctly choosing the best 
agents, yet, it also causes longer run times of the experiments.\\
\\
% Noise factor
Specific to the Cross-Entropy method, 
most authors report that the performance of the 
algorithm increases dramatically when the sampling 
distribution is associated with
a noise term. The different types of 
noise are described in section \ref{CrossEntropy}.
The noise term is adjusted in order to 
prevent the algorithm from reaching a local optimum.
The current research shows that noise terms of $\noise_\generation = 4$ and 
$\noise_\generation = max \left( 5 - \generation / 10 \right)$ 
\citep{thiery:09} produces the best results.
The constant noise (such as $\noise_\generation = 4$) ensures that the algorithm
never settles in a too small area from which it samples, and forces it to explore
solutions that are further away from the mean. The further the 
algorithm progresses, 
the less noise is assumed needed, as the mean should approach a global optimum. to
address this, the linear decreasing noise 
is applied as it will lower the noise term
as the algorithm progresses.\\
\\
For the various experiments, these 
parameters will be tuned for the specific purpose 
at hand. In the verification of the Cross-Entropy, the parameters are set 
to match those reported in similar papers (\cite{thiery:09}, \cite{szita:06}).
In the comparison of the two algorithms, the parameters will be set such that 
the Cross-Entropy operates under as 
similar conditions as CMA-ES, to ensure an unbiased 
comparison.

\subsection{Assesment of controller performance}
The performance of a one-piece controller has a very high variation,
and is in other research verified to be exponentially distributed.
As a result of the high variance of the controllers, the performance 
of single controllers are often presented along with a confidence interval
for the estimated mean score of the controller. The estimated mean of 
the controllers score is calculated by
\begin{align*}
\hat{\mean} &= \sum_{i=1}^{\populationSize} \individual_i
\end{align*}
Thus, the maximum likelihood estimation of the rate 
parameter\footnote{Note that $\lambda$ is in this context not,
the population size but instead the rate parameter for the
exponential distribution.}
of the distribution is given by
\begin{align*}
\hat{\lambda} = \frac{1}{\hat{\mean}}
\end{align*}
A confidence interval is found by the following
\begin{align*}
\frac{2\populationSize}{\hat{\lambda}\chi^{2}_{1-\frac{\alpha}{2},2\populationSize}}
<
\frac{1}{\lambda}
< 
\frac{2\populationSize}{\hat{\lambda}\chi^{2}_{\frac{\alpha}{2},2\populationSize}}
\end{align*}
However, for these experiments, an approximation for a $95\%$
confidence on lower and upper bound 
of the rate parameter $\lambda$ is used
\begin{align*}
\lambda_{low} &= 
\hat{\lambda} \left( 1 - \frac{1.96}{\sqrt{\populationSize}} \right)\\
\lambda_{upp} &= 
\hat{\lambda} \left( 1 + \frac{1.96}{\sqrt{\populationSize}} \right)
\end{align*}
By this, the $95\%$ confidence interval for the mean $\mean$ is
\begin{align*}
\frac{1}{\lambda_{low}} < \mean < \frac{1}{\lambda_{upp}}
\end{align*}
When a controllers score is presented as "$s \pm p$" this means 
has an empirical mean score of $s$ and a real mean that is with 
$95\%$ likelihood within $s \pm p$. \\
\comment{Add reference to exponential distribtutions}

\subsection{Verification of CE \label{varifyofce}}
Because the \shark library already contains an implementation of 
CMA-ES, but not an implementation of CE, we extended the library 
with our own implementation of the algorithm.\\
In order to verify the correctness of the implementation, 
we used the same experiments as used by 
Christophe Thiery and Bruno Scherrer \citep{thiery:09}. 
These experiments were used by Thiery and Scherrer to 
verify their own CE implementation with various types of noise correction. 
Therefore, we will perform the same experiments to verify our 
own contribution to the \shark library, by trying to achieve the same results.\\
\\
The setup is mirrored from the paper \citep{thiery:09}, 
with 100 agents ($\populationSize = 100$) per iteration,
and using the $\offspringNumber = 10$ best vectors
for the update step. After each iteration, 
an agent with the updated mean 
plays 30 games and the mean of these scores are recorded for the
learning curve.\\
During evaluation each agent plays one game, that is $\numberOfEvaluations = 1$.
A minor derivation from the figures present in \citep{thiery:09}, is 
the unit along the x-axis in the learning curve plots indicates 
the iteration number. As the experiments in this two algorithms
width variable population sizes are compared, the x-axis in all plots
indicated the number of tetris games played during the learning. In one 
generation $\populationSize$ agents each play $\numberOfEvaluations$ games and hence
adavance the x-axis by $\populationSize \numberOfEvaluations$.\\

\begin{figure}[H]
\begin{tikzpicture}
\meansPlot 
\end{tikzpicture}
\caption{Cross-Entropy mean performance \label{fig:cemean}}
\end{figure}

\begin{figure}[H]
\begin{tikzpicture}
\noNoisePlot
\end{tikzpicture}
\caption{No noise \label{fig:ceNoNoise}}
\end{figure}

\begin{figure}[H]
\begin{tikzpicture}
\constantNoisePlot 
\end{tikzpicture}
\caption{Constant noise \label{fig:ceCnstantNoise}}
\end{figure}

\begin{figure}[H]
\begin{tikzpicture}
\linearNoisePlot 
\end{tikzpicture}
\caption{Linear decreasing noise \label{fig:ceLinNoise}}
\end{figure}

Figure \ref{fig:ceNoNoise}, \ref{fig:ceCnstantNoise} and
\ref{fig:ceLinNoise} shows 10 runs of each noise type. Figure
\ref{fig:cemean} shows the mean graph for each of the noise types.
The goal of these experiments were to replicate the experiments 
reported in \citep{thiery:09}. As the results seen from our experiments
to a high degree resemble those reported by Thiery et. al, we conclude
that our Cross-Entropy implementation works similar to theirs.\\
\\
When evaluating the score of the agent we also want to compute the confidence
interval in verifying the implementation of CE. The mean agent plays 30 games
which leads to a confidence interval of $\pm36\%$ around the estimated mean,
which is similar to the confidence intervals in \citep{scherrer2009}.\\
By looking at the individual graphs for the different noise types 
(Figure \ref{fig:ceNoNoise}, \ref{fig:ceCnstantNoise} and
\ref{fig:ceLinNoise}), we get the following average scores.\\
\\
\textbf{Without noise (figure \ref{fig:ceNoNoise})}: The learning curve
stabilizes after 1,500 agents evaluated. And as it can be observed the
score variates much for the different executions between a score of 300 and 6,000
rows. This results in an average score of 1,400$\pm36\%$ rows.\\
\textbf{Constant noise (figure \ref{fig:ceCnstantNoise})}: 
The 10 executions reaches equivalent performances at some point, 
with a score between
54,000 and 154,000. This results in an average score of 105,000$\pm36\%$ rows.\\
\textbf{Linear decreasing noise (figure \ref{fig:ceLinNoise})}:
Most of the executions of this noise type settles around 200,000.
However, a single execution settled at a score of only around 5,000.
The mean performance of this noise type yielded a score of
120,000$\pm36\%$ \\

Based on the mean graphs and confidence interval compared to other papers, we can
hereby verify that our implementation of CE works as intended. Even though the
experiments with linear decreasing noise in this case seems to outperform
the constant noise, other runs with linear decreasing noise ended in a mean 
performance of only 90,000$\pm36\%$. Yet, the constant noise is both from our
own experiments, and described in other research, noted to be the most reliable
noise type for reaching high scoring controllers \citep{scherrer2009}. 
Due to this, the constant noise is used in the 
benchmarking against CMA-ES.\\

\subsection{Optimal settings 
for Cross Entropy \label{optimalsettingsce}}

Other researchers run the Cross Entropy algorithm with population size of
$\populationSize = 100$ and an offspring corresponding to 10\% of 
the population size, resulting in $\offspringNumber = 10$. As it's not 
discussed why this exact setting is applied, various settings of the 
Cross-Entropy was executed to asses the performance of other configurations
in our experiments.
The experiments includes different population sizes 
$\populationSize \in \{10, 22, 50, 100, 200\}$ and offspring 
sizes of either $10\%$ and $50\%$ (since the CMA algorithm by default
uses $50 \%$ offspring). 
A summary of the experiments can be seen in figure \ref{CEConfigTest}
on page \pageref{CEConfigTest}.

\begin{figure}[H]
\centering
\begin{tabular}{r r | r r r r}
$\populationSize$ & $\offspringNumber$ & mean & Q1 & Q2 & Q3\\
\hline
10 & $10\%$  & 704.6      & 7.2       & 48.3         & 430.3\\
10 & $50\%$  & 9,272.5    & 149.6     & 7626.5       & 16,919.9\\
22 & $10\%$  & 35,841.6   & 20,391.9  & 42,045.5     & 48,464.6\\
22 & $50\%$  & 52,887.4   & 23,531.9  & 42,161.0     & 83,144.1\\
50 & $10\%$  & 95,623.1   & 82,738.9  & 93,388.9     & 111,351.5\\
50 & $50\%$  & 69,130.7   & 52,511.0  & 64,351.6     & 91,488.6\\
\hdashline
100 & $10\%$ & 115,868.7  & 84,368.5  & 122,238.5    & 146,457.0\\
\hdashline
100 & $50\%$ & 22,910.4   & 4,037.7   & 14,353.7     & 47,215.9\\
200 & $10\%$ & 85,181.7   & 45,201.5  & 96,803.1     & 117,578.0\\
200 & $50\%$ & 946.4      & 585.0     & 802.5        & 1,267.7
\end{tabular}
\caption{Cross Entropy configuration test \label{CEConfigTest}}
\end{figure}

\comment{graphs to appendix}

The experiments with different population and offspring sizes
does not seem to support a choice for any other configuration 
than the mostly commonly used 
$\populationSize = 100$ and $\offspringNumber = 10$. \\
However, with a configuration of $\populationSize = 50$ and $\offspringNumber = 5$ convergence
is achieved faster. This means that the score limit is reached faster, which
results in longer computation time, than the $\populationSize = 100$ and $\offspringNumber = 10$
configuration. In other words, the $\populationSize = 100$ and $\offspringNumber = 10$ configuration 
is therefore preferred since it takes shorter computation time and the end-result is similar compared
to the $\populationSize = 50$ and $\offspringNumber = 5$ configuration, even though the latter 
configuration from our expriments converged faster. The experiments also apppears
to suffer from a high noise, yet both the mean and the quantiles favor the 
extensively tested Cross Entropy configuration of $\populationSize = 100$ and $\offspringNumber = 10$.

\comment{Add graphs of the two different configurations?}

\subsection{Step-size and lower bound for CMA}
Initially, the covariance matrix of CMA in generation $\generation = 0$
is the identity matrix. The initial step-size, $\sigma_0$, will hence in 
the first iteration scale the area in which the CMA algorithm searches.
As from section \ref{normalSamples}, it's known that the scale of the 
solutions has no impact on the scores. Hence, it's assumed that the initial 
step-size should not have any major impact on the results.

\begin{figure}[H]
\centering
\begin{tabular}{r | r r r r r}
$\sigma_0$ & mean & Q1 & Q2 & Q3\\
\hline
0.1 & 50769.3 & 21301.1 & 54588.7 & 73972.4\\
0.2 & 42290.6 & 32180.2 & 42290.6 & 49337.4\\
0.5 & 53893.7 & 14211.1 & 66773.0 & 85816.7\\
0.8 & 37557.7 & 1422.8  & 15450.8 & 93719.4\\
1.0 & 49537.9 & 31369.8 & 49537.4 & 58454.6
\end{tabular}
\caption{Results of CMA-ES with adjusted initial step-size \label{CMAInitialSigmaConfigTest}}
\end{figure}

For the initial experiments using CMA-ES, 
the only adjusted parameter is the initial 
step-size $\sigma_0$. The configurations of step-sizes were 
$\sigma_0 \in \{0.1, 0.2, 0.5, 0.8, 1.0\}$. As the table shows,
the final mean score does not seem to change with the initial step-size.
Furthermore, the adjustment of the step-sizes does not appear to 
have a drastic impact on the mean scores. However, based on both mean score and
quantiles, the best configuration seems to be $\sigma_0 = 0.5$. This is 
is also referred to as a typical initial setting in \citep{boumaza2009}.
Therefore, the conclusion remains that the initial step-size is not critical 
for the experiment.\\

As with the Cross Entropy method, to avoid too early convergence, a 
certain lower threshold for the variance should be applied when 
sampling vectors for solutions. In the Cross Entropy method, a constant 
noise term $z_t = 4$ was added to the variance for each component
of the sampled vectors. When the $i$'th component in cross entropy is
sampled as follows:
\begin{align*}
\individual_i &\sim \mathcal{N}\left(m, \sigma^{2}\right)\\
              &\sim \sigma \mathcal{N}\left(m, 1\right)
\end{align*}
Then $\sigma^{2} \geq 4$. To gain the same effect for the CMA, a lower bound 
is applied to the step-size. Such a bound is implemented in the Shark library
as the following, where the value of the lower bound is $l$.
\begin{align*}
\sigma  \lambda_n \geq l
\end{align*}
Where $\lambda_n$ is the lowest eigenvalue in the covariance matrix. 
When the vectors are sampled, the samples are scaled by the matrix $D$
containing the eignvalues of the covariance matrix. 
Hence, if $\sigma \lambda_n \geq l$, then the smallest scaling
that takes place is at least $l$. As the vectors are sampled
as: 
\reminder{Check if this is correct, 
and maybe place this in theoretical section.
Also make sure that variable names makes sense.}
\begin{align*}
\individual &\sim \mathcal{N}\left( m, \sigma^{2}C \right)
\end{align*}
To roughly resemble the constant noise configuration of Cross Entropy,
the dimension with the lowest variance must not drop below 4. 
This is achieved by setting a lower bound $l=2$. By setting this, 
the variance of the normal distribution in each dimension 
before rotation is ensured to be at least $\sigma^{2} = 4$
since $\sigma \mathcal{N}\left( 0, 1 \right) \sim 
\mathcal{N}\left( 0, \sigma^{2} \right)$.


\maybe{From this, it's assumed that the initial step-size, at least in this range,
does not have a significant impact, and the best of these runs, $\sigma_0$ is chosen
for first comparison.}



\comment{- Write stuff about our practical experiences with the step size and lower bound}\\
\comment{- Make sure to fully conclude that the initial step-size doesn't matter}






