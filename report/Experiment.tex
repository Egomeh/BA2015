\section{Experiments}

This section will walk through our experimental setup, our verification of CE and 
and a analysis/discussion of the results.

\subsection{MDP-Tetris}

When running the experiments, the source code of the MDP-Tetris
\citep{mdptetris} was used to emulate the Tetris games.
The source code is accompanied with files that describe the
various existing features. These files contains the identifiers of 
each feature to use, as well as two numbers respectively describing 
the agents reward function and how to evaluate a game over state. 
The number for the reward function has remained unchanged at $0$ 
during all experiments. The "game-over" evaluation was for the
Bertsekas feature set initially set to $0$. Setting the 
"game-over" evaluation to $0$ means that the agent will not 
distinguish between regular moves and moves that results in losing
the game. When running the experiments with this setting, a large portion
of the agents never exceeded a zero mean score. However, setting the value
to $-1$, meaning that a "game-over" move yields $-\infty$ reward, 
none of the experiments got stuck on only zero scores. An example
of the layout of the feature file can be seen in figure \ref{fig:featfile}.
\begin{figure}[h!]
\centering
\begin{lstlisting}
0    <- Describes the reward function
-1   <- Actions leading to game over is avoided at all cost
22   <- The policy contains 22 features
8 0  <- The feature with id 8 initially has weight 0
...  <- The remaining 21 features
\end{lstlisting}
\caption{Example of a file that describes a feature set. \label{fig:featfile}}
\end{figure}

 
\subsection{Normalization of samples}
As mentioned by some authors \citep{boumaza2009}, the vector that
describes the agent can very well be normalized such that the vector
is a point that lies on the $\dimensions$-dimensional hypersphere.\\
\\
The reason for this lies in the nature of the evaluation function.
When the controller chooses an action, it will evaluate all the 
possible actions possible with the current piece. It will use the 
value function $\valueFunction$ of each state $\gameState_i$ and 
choose the state with the highest value from the value function.
Thus, if the states are ordered such that:
\begin{align*}
\valueFunction \left(  \gameState_1 \right) 
> \dots 
> \valueFunction \left( \gameState_\populationSize \right)
\end{align*}

The agent then chooses the action that transitions from the current state 
to state $\gameState_1$.\\
Since the value function assess the state by the following:
\begin{align*}
\valueFunction (\gameState ) &= 
\sum_{i=1}^{\dimensions} \weight _{i}\feature _{i}(\gameState )
\end{align*}

Then scaling the input of the agent, the weight vector $\weight$, by a
number $a \in \mathbb{R}, \ a > 0$ the assessment is changed by:
\begin{align*}
\sum_{i=1}^{\dimensions} a \weight _{i}\feature _{i}(\gameState ) &= 
a\sum_{i=1}^{\dimensions} \weight _{i}\feature _{i}(\gameState )\\
&= a \valueFunction \left( \gameState \right)
\end{align*}

And the ordering remains:
\begin{align*}
a \valueFunction \left(  \gameState_1 \right) 
> \dots 
> a \valueFunction \left( \gameState_\populationSize \right)
\end{align*}

Thus the order of the value functions of 
each state does not change, and the same $\gameState_1$
is still chosen for any $a \in \mathbb{R}, \ a > 0$.\\
\\
To verify this, the Tetris objective function was executed with the
same vector and the same seed for the random generator with a scale
$a \in \{0.1, 0.2,0.3, \dots, 9.8,9.9,10.0\}$, and the agent scored 
exactly the same for each scale.\\
\\
This can be used in experiments for various reasons. As reported 
in \citep{boumaza2009}, normalizing the samples will 
prevent CMA-ES from diverging in step size,
and it can prevent loosing precision if the magnitude of weight 
vector becomes larger than feasible for the used floating 
point number and avoids size limitations.



\subsection{Setup}

When executing the experiments, various parameters each have 
an impact on the final result of the learning curve. Thus, the parameters
are adjusted, first to match the experiments run by other researchers, 
and later to conduct as fair as possible comparisons between 
Cross-Entropy and CMA-ES.\\
\\
% agents
The amount of vectors sampled in each generation $\populationSize$
has a high impact on the algorithm performance. By setting $\populationSize$
high, more policies are evaluated per iteration, and leads to a more thorough 
exploration of the search space. Thus the higher $\populationSize$ increases the
chances of finding a better mean for the next iteration.
However, higher $\populationSize$ also results in the
need for more evaluations per iteration. The goal for 
tuning this parameter is then
to set $\populationSize$ high enough to ensure 
exploration of good solutions, and yet 
low enough to avoid unnecessary evaluations.\\
In the implementation of CMA-ES from \shark , 
the algorithm  itself determines
the value of $\populationSize$ according to the 
size of the search space. 
Cross-Entropy however, does not seem to have a 
general rule for this parameter,
so this value is manually adjusted to fit the 
problem as well as possible.\\
\\
% offspring
As both of the optimizing algorithm uses a subset of the sampled vectors
from a generation to update the distribution parameters, the number of 
offspring $\offspringNumber$ influences how the next generation is sampled.
By setting the value too high, the algorithm risks ceasing to progress any 
further since the updated mean would be too close to the previous one to 
significantly make a difference. By setting the value too low,
the risk of reaching a local optimum increases since the high-scoring agents
might have reached their high performance by chance.\\
The CMA-ES itself manages setting $\offspringNumber$ and Cross-Entropy
is set according to the problem. Most authors that uses Cross-Entropy for Tetris
sets the offspring size to $10\%$ of population size, that is 
$\offspringNumber = \lfloor 0.1 \cdot \populationSize \rfloor $.\\
\\
% Number of games per iteration
The number of games, $\numberOfEvaluations$, 
is the number of games  which each agent 
plays in each iteration. An agent's score is defined as the 
mean of the score of these 
$\numberOfEvaluations$ games.
We want this value low as possible, because as with the number of
agents, $\populationSize$, The number of games, $\numberOfEvaluations$, 
is another major factor in the run-time of the algorithm.
As Tetris is stochastic by nature, the score deviates a lot, 
even when the
same agent with the same policy plays multiple games. 
Hence, when assessing the true
performance of a policy it's rarely enough to play just few games. Thus, setting 
$\numberOfEvaluations$ high increases the likelihood of correctly choosing the best 
agents, yet, it also causes longer run times of the experiments.\\
\\
% Noise factor
Specific to the Cross-Entropy method, 
most authors report that the performance of the 
algorithm increases dramatically when the sampling 
distribution is associated with
a noise term. The different types of 
noise are described in section \ref{CrossEntropy}.
The noise term is adjusted in order to 
prevent the algorithm from reaching a local optimum.
The current research shows that noise terms of $\noise_\generation = 4$ and 
$\noise_\generation = max \left( 5 - \generation / 10 \right)$ 
\citep{thiery:09} produces the best results.
The constant noise (such as $\noise_\generation = 4$) ensures that the algorithm
never settles in a too small area from which it samples, and forces it to explore
solutions that are further away from the mean. The further the 
algorithm progresses, 
the less noise is assumed needed, as the mean should approach a global optimum. to
address this, the linear decreasing noise 
is applied as it will lower the noise term
as the algorithm progresses.\\
\\
For the various experiments, these 
parameters will be tuned for the specific purpose 
at hand. In the verification of the Cross-Entropy, the parameters are set 
to match those reported in similar papers (\cite{thiery:09}, \cite{szita:06}).
In the comparison of the two algorithms, the parameters will be set such that 
the Cross-Entropy operates under as 
similar conditions as CMA-ES, to ensure an unbiased 
comparison.


\subsection{Verification of CE}
Because the \shark library already contains an implementation of 
CMA-ES, but not an implementation of CE, we extended the library 
with our own implementation of the algorithm. 
This ensures as many similar conditions as possible for 
the two optimization algorithms as possible.\\
In order to verify the correctness of the implementation, 
we used the same experiments as used by 
Christophe Thiery and Bruno Scherrer \citep{thiery:09}. 
These experiments were used be Thiery and Scherrer to 
verify their own CE implementation with various types of noise correction. 
Therefore, we will perform the same experiments to verify our 
own contribution to the \shark library, by trying to achieve the same results.\\
\\
The setup is mirrored from the paper \citep{thiery:09}, 
with 100 agents ($\populationSize = 100$) per iteration. 
The 10 best agents ($\offspringNumber = 10$) will be used 
to update gaussian distribution. After each iteration, 
an agent with the updated mean 
plays 30 games and the mean of these scores are recorded for the
learning curve.\\
During evaluation each agent plays one game, that is $\numberOfEvaluations = 1$.\\
\begin{figure}[h!]
\begin{tikzpicture}
\meansPlot 
\end{tikzpicture}
\caption{Cross-Entropy mean performance \label{fig:cemean}}
\end{figure}

\begin{figure}[h!]
\begin{tikzpicture}
\noNoisePlot
\end{tikzpicture}
\caption{No noise \label{fig:ceNoNoise}}
\end{figure}


\begin{figure}[h!]
\begin{tikzpicture}
\constantNoisePlot 
\end{tikzpicture}
\caption{Constant noise \label{fig:ceCnstantNoise}}
\end{figure}


\begin{figure}[h!]
\begin{tikzpicture}
\linearNoisePlot 
\end{tikzpicture}
\caption{Linear decreasing noise \label{fig:ceLinNoise}}
\end{figure}

Figure \ref{fig:ceNoNoise}, \ref{fig:ceCnstantNoise} and
\ref{fig:ceLinNoise} shows 10 runs of each noise type. Figure
\ref{fig:cemean} shows the mean graph for each of the noise types.
The goal of these experiments were to replicate the experiments 
reported in \citep{thiery:09}. As the results seen from our experiments
to a high degree resemble those reported by Thiery et. al, we conclude
that our Cross-Entropy implementation works similar to theirs.

\comment{Technical reason for why the results are accepted.}


\subsection{Comparison between CMA-ES and CE}

\comment{add experiments that compare CMA-ES to cross entropy}

\subsection{Results}

\comment{Show results from comparison experiments}

\subsection{Analysis and discussion}

\comment{Discuss the results of the comparison experiments}

