\section{Experiments}

This section will explore our experimental setup, our verification of CE and 
and a analysis/discussion of the results.

\subsection{MDP-Tetris}

When running the experiments, the source code of the MDP-Tetris
\citep{mdptetris} was used to emulate the Tetris games.
The source code is accompanied with files that describe the
various existing features. These files contains the identifiers of 
each feature to use, as well as two numbers respectivly describing 
the agents reward function and how to evaluate a game over state. 
The number for the reward function has remained unchanged at $0$ 
during all experiments. The "game-over" evaluation was for the
Bertsekas featureset initially set to $0$. Setting the 
"game-over" evaluation to $0$ means that the agnet will not 
destinguish between regular moves and moves that results in losing
the game. When running the experiments with this setting, a large portion
of the agents never exceeded a zero mean score. However, setting the value
to $-1$, meaning that a "game-over" move yieds $-\infty$ reward, 
none of the experiments got stuck on only zero scores.

\subsection{Setup}

When executing the experiments, various parameters each have 
impact on the final result of the learning curve. Thus, the parameters
are adjusted, first to match the experiments run by other authors, and later
to conduct as fair as possible comparisons between Cross-Entropy and 
CMA-ES.\\
\\
% agents
The amount of vectors sampled in each generation $\populationSize$
has obvious impact on the algorithm performance. By setting $\populationSize$
high, more policies are evaluated per iteration, and leads to a more thorough 
exploration of the search space. Thus the higher $\populationSize$ increases the
chances of finding a better mean for the next iteration.
However, higher $\populationSize$ also results in the
need for more evaluations per iteration. The goal for 
tuning this parameter is then
to set $\populationSize$ high enough to ensure 
exploration of good solutions, and yet 
low enough to avoid unnecessary evaluations.\\
In the implementation of CMA-ES used in \shark , 
the algorithm  itself determines
the value of $\populationSize$ according to the 
size of the search space. 
Cross-Entropy however, does not seem to have a 
general rule for this parameter,
so this value is manually adjusted to fit the 
problem as well as possible.\\
\\
% offspring
As both of the optimizing algorithm uses a subset of the sampled vectors
from a generation to update the distribution parameters, the number of 
offspring $\offspringNumber$ influences how the next generation is sampled.
By setting the value too high, the algorithm risks ceasing to progress any 
further since the new mean would be too close to the previous mean to 
significantly move the mean. By setting the value too low,
we risk reaching a local optimum since the high-scoring agents 
might have been lucky in "one-time" occurring situations.\\
Once again, the CMA-ES itself manages setting $\offspringNumber$ and Cross-Entropy
is set according to the problem. Most authors that uses Cross-Entropy for Tetris
sets the offspring size to $10\%$ of population size, that is 
$\offspringNumber = \lfloor 0.1 \cdot \populationSize \rfloor $.\\
\\
% Number of games per iteration
The number of games, $\numberOfEvaluations$, is the number of games  which each agent 
has to play in each iteration. An agent's score from a given
iteration/generation is defined as the mean of the score of these 
$\numberOfEvaluations$ games.
We want this value low as possible, because as with the number of
agents, $\populationSize$, The number of games, $\numberOfEvaluations$, 
is another major factor which 
impacts the run-time of the algorithm.
As Tetris is stochastic by nature, the score deviates a lot, 
even when the
same agent with the same policy plays multiple games. 
Hence, when assessing the true
performance of a policy it's rarely enough to play just few games. Thus, setting 
$\numberOfEvaluations$ high increases the likelihood of correctly choosing the best 
agents, yet, it also causes longer run times of the experiments.\\
\\
% Noise factor
Specific to the Cross-Entropy method, 
most authors report that the performance of the 
algorithm increases dramatically when the sampling 
distribution is associated with
a random noise. The different types of 
noise are described in section \ref{CrossEntropy}.
The noise term is adjusted in order to 
prevent the algorithm from reaching a local optimum.
The current research shows that noise terms of $\noise_\generation = 4$ and 
$\noise_\generation = max \left( 5 - \generation / 10 \right)$ \citep{thiery:09}.
The constant noise (such as $\noise_\generation = 4$) ensures that the algorithm
never settles in a too small area from which it samples, and forces it to explore
solutions that are further away from the mean. The further the algorithm gets, 
the less noise is assumed needed, as the mean should approach a global optimum. to
address this, the linear decreasing noise 
is applied as it will lower the noise term
as the algorithm progresses.\\
\\
For the various experiments, these 
parameters will be tuned for the specific purpose 
at hand. In the verification of the Cross-Entropy, the parameters are set 
to match those reported in similar papers (\cite{thiery:09}, \cite{szita:06}).
In the comparison of the two algorithms, the parameters will be set such that 
the Cross-Entropy operates under as 
similar conditions as CMA-ES, to ensure an unbiased 
comparison.


\subsection{Verification of CE}
Because the \shark library already contains an implementation of 
CMA-ES, but not an implementation of CE, we extended the library 
with our own implementation of the algorithm. 
This ensures as many similar conditions as possible for 
the two optimization algorithms as possible.\\
In order to verify the correctness of the implementation, 
we used the same experiments as used by 
Christophe Thiery and Bruno Scherrer \citep{thiery:09}. 
These experiments were used be Thiery and Scherrer to 
verify their own CE implementation with various types of noise correction. 
Therefore, we will utilize the same experiments to verify our 
own contribution to the \shark library, by trying to achieve the same results.\\
\\
The setup is mirrored as in the paper \citep{thiery:09}, 
with 100 agents ($\populationSize = 100$) per iteration. 
The 10 best agents ($\offspringNumber = 10$) will be used 
to update gaussian distribution. After each iteration, 
an agent with the mean weights from the $\offspringNumber$ best agents, 
play 30 games.\\
Each agent is during evaluation plays one game, that is $\numberOfEvaluations = 1$.\\
\\
\begin{figure}[h]
\comment{FIGURE MISSING}
\caption{Cross-Entropy mean performance \label{fig:cemean}}
\end{figure}

\begin{figure}[h]
\caption{No noise \label{fig:ceNoNoise}}
\end{figure}


\begin{figure}[h]
\begin{tikzpicture}
\begin{axis}[%
    xlabel={xlabel},
    ylabel={ylabel}]
\end{axis}
\end{tikzpicture}
\caption{Constant noise \label{fig:ceCnstantNoise}}
\end{figure}


\begin{figure}[h]
\comment{FIGURE MISSING}
\caption{Linear decreasing noise \label{fig:ceLinNoise}}
\end{figure}

Figure \ref{fig:ceNoNoise}, \ref{fig:ceCnstantNoise} and
\ref{fig:ceLinNoise} shows 10 runs of each noise type. Figure
\ref{fig:cemean} shows the mean graph for each of the noise types.
The goal of these experiments were to replicate the experiments 
reported in \citep{thiery:09}. As the results seen from our experiments
to a high degree resemble those reported by Theiery et. al, we conclude
that our Cross-Entropy implementation works similar to theirs.
\\
\comment{Conclusion - is CE good enough? cyka blyat}

\subsection{Results}

What were the results of the experiment? This section will
present theoutcome of the experiment, or refer to data.

\subsection{Analysis}

How do CE and CMA-ES compare from our data? Can we say anything 
which performs better?

