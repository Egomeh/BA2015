\subsubsection{Initial comparison - Default settings \label{sec:initialCompare}}
The initial comparison is meant to show how the CMA-ES performs against the Cross-entropy method
under default settings. The default setting for CMA-ES are those set by default
by Shark. The settings for the Cross-entropy method are those that are said to perform best
and most
consistently by other researchers.
Hence, the goal of this comparison is to get an initial 
idea of how the Shark implementation of
CMA-ES compares to the Cross-entropy method with settings
used by other researchers \citep{thiery:09}. Table \ref{tab:initialComparisonSettings}
summarises the settings of each algorithm used in the initial experiment.
\begin{table}[H]
\centering
\begin{tabular}{l r}
Optimizer & CMA-ES\\
Evaluations/agent & 8000\\
Learning Games & 30\\
Population size& 13\\
Parent size & 6\\
Games per Agent & 1\\
Tetris Type & Normal\\
\hline
Recombination Type & SUPERLINEAR\\
Initial Sigma & 0.5\\
Lower bound & NONE\\
\end{tabular}
\quad
\begin{tabular}{l r}
Optimizer & Cross Entropy\\
Evaluations/agent & 8000\\
Learning Games & 30\\
Population size & 100\\
Parent size & 10\\
Games per Agent & 1\\
Tetris Type & Normal\\
\hline
Sigma & 100\\
Noise Type & Constant\\
Noise & 4
\end{tabular}
\caption{Optimized Cross-entropy method and CMA-ES for comparison experiments \label{tab:initialComparisonSettings}}
\end{table}


\textbf{Results}

Figure \ref{fig:CMA_VS_CE_00} shows the mean learning curves of the two algorithms,
and table \ref{table:initialResultTable} shows the results from the final iteration of each algorithm.
As figure \ref{fig:CMA_VS_CE_00} reveals that CMA-ES converges faster,
but reaches a local optimum at around 2,000 games played. Meanwhile the Cross-entropy method has a 
slower convergence but reaches a better mean score compared to CMA-ES at around 5,500
agents evaluated. In detail, CMA-ES on average reaches a score of 50,000 rows, and
the Cross-entropy method reaches a score of 100,000.\\

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.8]{plots/cmaCePlot}
\end{center}
\caption{Initial comparison between CMA-ES and Cross-entropy method \label{fig:CMA_VS_CE_00}}
\end{figure}

\begin{table}[H]
\centering
\small
\begin{tabular}{l r r r r}
Optimizer & Mean & Q1 & Q2 & Q3\\
\hline
CMA-ES  & $57783.0$ & $10269.9$ & $59774.5$ & $100384.1$\\
The Cross-entropy method & $116289.4$ & $85230.9$ & $125329.5$ & $138715.5$\\
\end{tabular}
\caption{Results from last iteration of the curves in figure \ref{fig:CMA_VS_CE_00}
\label{table:initialResultTable}}
\end{table}

\textbf{Analysis and discussion}

These results clearly defy our initial hypothesis as we predicted
for CMA-ES to outperform Cross-entropy method, due to its more sophisticated nature. 
One reason for this outcome could possibly be that
CMA-ES has a very little population size compared to Cross-entropy method,
which could be a decisive lack as the objective function is noisy with 
a high variance.\\

\begin{changebar}
To perform a comparison between the two algorithms on the same terms,
we saw from earlier experiments that both algorithms benefits from 
adjusted settings. Thus, another experiment is required, where both algorithms
are adjust to have the best possible settings to ensure that both algorithms
perform as well as possible. Among the adjusted parameters are the following.\\
\\
\textit{Enlargment of population size}\\
Our first experiment clearly shows that the CMA-ES
is outperformed by the Cross-entropy method.
The CMA-ES does by default only have a relatively small
population size (around 13) compared to the Cross-entropy methods
default (100). From the earlier experiments, we saw that 
the Cross-entropy method does benefit from a higher populations size,
and to CMA-ES, the higher population size appears critical
to it's performance. Thus, the population size is increased for 
both algorithm in our final experiment.\\
\\
\textit{Evaluate each agent multiple times}\\
From our experiments, the Cross-entropy method does not
seem to require evaluation each search point more than once
and thus, the number of evaluations remains at 1 per
search point. For the CMA-ES however, with a 
ranking mechanism that strongly emphasizes 
the best search points in a population, a firm
ranking is important. Our experiments suggested that
evaluation each search point 5 times appears to be optimal 
for the CMA-ES.\\
\\
\textit{Change the recombination type}\\
As described in section \ref{CMAtheory}, 
the CMA-ES is not bound to update its 
new mean to just the centroid of the selected 
vectors. Instead, it can weight better solutions
more heavily when moving its mean. When doing so,
it risks biasing search points that appear to be better 
but in reality, just by faulty ranking, should
not be considered a good agent. Our experiments 
did however suggest that with 5 evaluations per search agent,
the super linear recombination was the best option.\\
\\
None of the experiments seen so far has allowed a 
consistent mean score of more 
than 200,000. This brings the concern 
hat it might be the objective function
that poses a natural limit on the score into consideration.
In this case the objective function
models playing Tetris with the Bertsekas feature set. 
It is unknown to us
whether it's possible to construct an agent 
with a mean score of more than
200,000 lines on average. 
Thus we cannot know if convergence 
of the algorithms are caused by the feature set or limitations in the
algorithms.
\end{changebar}
