\subsubsection{Tuned comparison - Optimized settings \label{tunedComparison}}
In this section we reveal the results of the experiments comparing 
the optimized Cross-entropy method
against the
optimized CMA-ES obtained from the sections, \ref{optimalsettingsce} and \ref{optimalsettingscma}.
The chosen optimal settings for each of the algorithms is specified in the following tables
\begin{table}[H]
\centering
\begin{tabular}{l r}
Optimizer & CMA-ES\\
Evaluations/agent & 40000\\
Learning Games & 30\\
Population size& 50\\
Parent size & 25\\
Games per Agent & 5\\
Tetris Type & Normal\\
\hline
Recombination Type & SUPERLINEAR\\
Initial Sigma & 0.5\\
Lower bound & 2.0\\
\end{tabular}
\quad
\begin{tabular}{l r}
Optimizer & Cross Entropy\\
Evaluations/agent & 40000\\
Learning Games & 30\\
Population size & 200\\
Parent size & 50\\
Games per Agent & 1\\
Tetris Type & Normal\\
\hline
Sigma & 100\\
Noise Type & Constant\\
Noise & 4
\end{tabular}
\caption{Optimized Cross-entropy method and CMA-ES for comparison experiments}
\end{table}
The goal of this experiment is to derive a final conclusion
on any performance difference between CMA-ES and the Cross-entropy method.
Thus this experiment is meant to clarify if any difference performance difference
is present when the two optimization algorithms are applied to learning Tetris.\\

\textbf{Results}\\
Each of the individual 30 learning curves from the Cross-entropy method
and CMA-ES are shown in figure \ref{fig:tunedAll}. Figure \ref{fig:TunedMean} shows the the
learning curve means from each algorithm. Figure \ref{fig:TunedMean} also contains 
labels that depicts the likelihood of whether the two score distributions actually differ.
These results are computed by taking the individual scores after playing respectively 
12,000 and 40,000 games played and using the Mann-Whitney-Wilcoxon test
as in section \ref{sec:CMAInitialStepSize}. The plot in figure \ref{fig:TunedMean}
reveals that the Cross-entropy method converges much faster than CMA-ES, as it 
appears to have reached stability after playing between 15,000 and 20,000 games. However,
CMA-ES reaches similar scores only during the very last iterations, namely around 35,000 games.
\begin{figure}[H]
\begin{tabular}{@{}c@{}c@{}}
Cross Entropy & CMA-ES\\
\includegraphics[scale=1]{plots/ce_tuned_all} &
\includegraphics[scale=1]{plots/cma_tuned_all}
\end{tabular}
\caption{30 runs with the optimal settings for the Cross-entropy method and CMA-ES \label{fig:tunedAll}}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{plots/TunedPlot.pdf}
\caption{Mean plot of the optimal settings experiments for the Cross-Entropy method and CMA-ES. 
The marking of $p$ indicates likelihood that the two mean graphs are from the same 
distribution.\label{fig:TunedMean}}
\end{figure}

\begin{table}[H]
\centering
\small
\begin{tabular}{l r r r r}
Optimizer & Mean & Q1 & Q2 & Q3\\
\hline
CMA-ES  & $163681.7$ & $145141.0$ & $157779.0$ & $184278.0$\\
Cross-entropy method & $152488.7$ & $133287.9$ & $153799.5$ & $166625.0$\\
\end{tabular}
\caption{Results from last iteration of the curves in figure \ref{fig:TunedMean}
\label{table:tunedResultTable}}
\end{table}
\vspace{0.5cm}
\textbf{Analysis and discussion}\\
Using our found optimal settings for both algorithms reveals some change in the 
development of the learning curves. We now see that CMA-ES converges slower than
the Cross-entropy method when using these optimal settings. 
The optimized settings for Cross-entropy method yields a much slower convergence
with some improvement. The experiment in section \ref{sec:initialCompare}, the Cross-entropy
methods's final score is on average around 115,000. With the optimal settings
the Cross-entropy method scores an average around 150,000. Thus, the Cross-entropy 
method does perform slightly better with our new-found settings.
CMA-ES also suffers from much slower convergence 
when using the optimized settings, but in return, it becomes able to compete with
the scores of the Cross-entropy method. The results seen when using the Cross-entropy
method  might
score slightly better with our found optimal settings, however these settings
brings the cost of a much longer learning time. The optimized settings show
an increment of the average score of around 35,000, this also causes convergence to
take place much later. With the original settings, the Cross-entropy method converges 
after around 7,000 games played, and with the optimal settings, the convergence 
occurs around 15,000-20,000 games played.
CMA-ES benefits greatly from the new settings in terms of final score.
With the default settings from Shark, the average final score of CMA-ES was around
50,000-60,000 which is increased to around 160,000 with the optimal settings.
The CMA-ES does however appear to have a much prolonged learning time
as a cause of the optimal settings, which must be said to be a significant trade-off.\\
\\
As described previously, we have carried out two tests using the Wilcoxon test, at two 
points on the learning curves. The first point is when the algorithms have played 12,000 
games and the second is at the final iteration after 40,000 games. The first point is chosen at a point 
where the two learning curves appeared greatly different and where the Cross-entropy method
is starting to stabilize. This point is chosen to determine if we can firmly tell
whether one algorithm learns faster than the other. Again, the null hypothesis of the
test is that the two algorithms on average gains the same scores after playing 
a certain number of games.
After 12,000 games played, the Wilcoxon test returns $p=4.528 \cdot 10^{-11}$. We reject the
null hypothesis at any $p < 0.05$, and therefore we conclude that the Cross-entropy method
in this case indeed do converge much faster than the CMA-ES. After playing the final number of games,
the Wilcoxon test gives $p=0.1471$, which means that we cannot reject the null hypothesis 
that the two algorithms reaches different scores at the final iteration. 
Thus, with this experiment, 
we cannot draw the conclusion that either reaches the highest scores. 
As the final score does not show a significant difference, the Cross-entropy method will 
immediately emerge as the algorithm of choice for learning Tetris, at least in our case
with the one-piece controller model, and using the Bertsekas feature set.







