\subsection{Optimal settings 
for CMA \label{optimalsettingscma}}



\subsubsection{Initial step-size}
Initially, the covariance matrix of CMA in generation $\generation = 0$
is the identity matrix. The initial step-size, $\sigma_0$, will hence in 
the first iteration scale the area in which the CMA algorithm searches.
As from section \ref{normalSamples}, it's known that the scale of the 
solutions has no impact on the scores. Hence, it's assumed that the initial 
step-size should not have any major impact on the results.

\begin{figure}[H]
\centering
\begin{tabular}{r | r r r r r}
$\sigma_0$ & mean & Q1 & Q2 & Q3\\
\hline
0.1 & 50769.3 & 21301.1 & 54588.7 & 73972.4\\
0.2 & 42290.6 & 32180.2 & 42290.6 & 49337.4\\
0.5 & 53893.7 & 14211.1 & 66773.0 & 85816.7\\
0.8 & 37557.7 & 1422.8  & 15450.8 & 93719.4\\
1.0 & 49537.9 & 31369.8 & 49537.4 & 58454.6
\end{tabular}
\caption{Results of CMA-ES with adjusted initial step-size \label{CMAInitialSigmaConfigTest}}
\end{figure}

For the initial experiments using CMA-ES, 
the only adjusted parameter is the initial 
step-size $\sigma_0$. The configurations of step-sizes were 
$\sigma_0 \in \{0.1, 0.2, 0.5, 0.8, 1.0\}$. As the table shows,
the final mean score does not seem to change with the initial step-size.
Furthermore, the adjustment of the step-sizes does not appear to 
have a drastic impact on the mean scores. However, based on both mean score and
quantiles, the best configuration seems to be $\sigma_0 = 0.5$. This is 
is also referred to as a typical initial setting in \citep{boumaza2009}.
Therefore, the conclusion remains that the initial step-size is not critical 
for the experiment.\\


\subsubsection{Lower bound}

As with the Cross Entropy method, to avoid too early convergence, a 
certain lower threshold for the variance should be applied when 
sampling vectors for solutions. In the Cross Entropy method, a constant 
noise term $z_t = 4$ was added to the variance for each component
of the sampled vectors. When the $i$'th component in cross entropy is
sampled as follows
\begin{align*}
\individual_i &\sim \mathcal{N}\left(m, \sigma^{2}\right)\\
              &\sim \sigma \mathcal{N}\left(m, 1\right)
\end{align*}
Then $\sigma^{2} \geq 4$. To gain the same effect for the CMA, a lower bound 
is applied to the step-size. Such a bound is implemented in the Shark library
as the following, where the value of the lower bound is $l$.
\begin{align*}
\sigma  \lambda_n \geq l
\end{align*}
Where $\lambda_n$ is the lowest eigenvalue in the covariance matrix. 
When the vectors are sampled, the samples are scaled by the matrix $D$
containing the eignvalues of the covariance matrix. 
Hence, if $\sigma \lambda_n \geq l$, then the smallest scaling
that takes place is at least $l$. As the vectors are sampled
as
\reminder{Check if this is correct, 
and maybe place this in theoretical section.
Also make sure that variable names makes sense.}
\begin{align*}
\individual &\sim \mathcal{N}\left( m, \sigma^{2}C \right)
\end{align*}
To roughly resemble the constant noise configuration of Cross Entropy,
the dimension with the lowest variance must not drop below 4. 
This is achieved by setting a lower bound $l=2$. By setting this, 
the variance of the normal distribution in each dimension 
before rotation is ensured to be at least $\sigma^{2} = 4$
since $\sigma \mathcal{N}\left( 0, 1 \right) \sim 
\mathcal{N}\left( 0, \sigma^{2} \right)$.


\maybe{From this, it's assumed that the initial step-size, at least in this range,
does not have a significant impact, and the best of these runs, $\sigma_0$ is chosen
for first comparison.}

\comment{- Write stuff about our practical experiences with lower bound}\\
\comment{- Make sure to fully conclude that the initial step-size doesn't matter}


\subsubsection{Configuration of CMA}

In previous sections we focused on tuning Cross Entropy for the Tetris 
problem. Whereas we deliberately chose not to tune CMA due to its implementation 
into the Shark library \citep{shark08}. However, experiments with the "out of 
the box" CMA from Shark, with default settings vs the tuned Cross Entropy
resulted in CMA reaching convergence very fast but not achieving the same point
limit as Cross Entropy.\\
By adjusting the population size to that similar of Cross Entropy, we are able
to get a fair comparison between the two algorithms, given each generation will
contain the same number of agents. By setting the population and offspring size
to the same values, we in effect test if the covariance matrix and the step-size
control has a impact on the algorithm performance compared to Cross Entropy
which does not have the features.\\
Furthermore, CMA also has a unique formula for calculating the updated mean,
called the 'Recombination type' \ref{CMAtheory}. Where the recombination type
determines how much influence each of the offspring vectors has on the next
generation. Built into the CMA algorithm is three methods of recombination. 
\begin{itemize}
\item EQUAL, Each of the offspring vectors has equal influence in the generated mean. Each has $w_i = 1$.
\item LINEAR, The best of the offspring vectors has more influence. 
\item SUPERLINEAR, The vectors are weighted with a logarithmic equation. $w_i = \frac{w_i'}{\sum_{j=1}^{\mu} w_j'^+}$
\end{itemize}
As default, CMA uses Super Linear recombination. However, Tetris is a problem
with multiple local optimums in its solution space. This means, though a vector may be the best in its generation, it could be a nearby local optimum. Therefore, Super Linear recombination may not be the optimal recombination type for the Tetris problem.\\
\comment{Discuss games per. agent, featuresets, Tetris difficulties, total agent evaluation / stopping criteria, parent size.}
\\
\comment{Was about to list the experiment setup (population/offspring with recombination type - wait for Oswin conversation to end), will perform experiments with hard tetris}\\
\\
\comment{Reference to CMA-ES section needs to be fixed}\\
\comment{Weights for recombination could have a symbol? (change policy weight
symbol?)}\\
\comment{Find Linear combination type formula of change format of itemize list}

\textbf{Results}

%\begin{tabular}{l l | l l l }
% & \multicolumn{1}{r|}{$\populationSize / \offspringNumber$} & (22/8) & (50/10) %& (100/10)
%\multicolumn{1}{l}{Recombination type} & &  &  & \\
%\hline
%\multicolumn{2}{l|}{Equal / $w_i = \frac{1}{\sum_{j=1}^{\offspringNumber} w_j}$}}  & & &
%\end{tabular}

\textbf{Analysis and discussion}


\subsubsection{PopulationSize}


\subsubsection{Recombination type}



\subsubsection{Games per agent}









