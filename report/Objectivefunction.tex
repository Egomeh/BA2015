\section{Objective function}

The optimizing algorithms used in this thesis all attempts to 
optimize a certain function. In this case, the optimization 
function aims to develop the best possible Tetris playing agent.
Thus, the value of the objecive funtion describes and estimated 
performance of input agent. The objective function is then 
an abstraction to the Tetris emulator that will evaluate 
the agent by letting the agent play a pre-configured number 
of games.\\
\\
When the Tetris simulator plays Tetris, the internal decision process
of the tetris controller is configured with a set of parameters that remain
fixed across the entire game. These parameters are the weights associated 
with each feature function that is considered by the controller. The features $\feature _{i}$
each map a state of the game $\gameState$ to a real value. An overview of the exact mappings
can be seen in table 1 in \citep{scherrer2009:b}. How much each of these mappings
should count in on the final evaluation of the board is determined by $\weight _{i}$
denoting the weight of the $i$-th feature. Finally, the function to assess the value 
of current board state $\valueFunction$, with $\dimensions$ features functions present, can be expressed as:

\begin{align*}
\valueFunction (\gameState ) &= \sum_{i=1}^{\dimensions} \weight _{i}\feature _{i}(\gameState )
\end{align*}

For each possible action the controller may take, it simulates 
the action and evaluate the value of the board for the new state. 
The final action is the one that yields the state of the highest value.
The performance of the controller is hence directly tied to the 
features and weights in the evaluation function. To adjust these controllers,
one can either change the set of used feature functions, or as the 
optimization algorithms will do, change the weighting of the features.
For the experiments, the feature sets remain fixed, and the task of the
optimization algorithm applied is to tune the set of weights in order 
to maximize the performance of the controller.\\
\\
The objective function then accepts a vector of values for each weight
and configures an agent with the weights corresponding to the entries in
the input vector. It then plays $\numberOfEvaluations$ games with the
agent and reports the mean score.




% evaluating a specific agent
Now that the new generation has been created, 
it's time for each agent to play g games of Tetris.\\
For a given Tetris game, we say that a game consists 
of a range of states. A state is defined as, when an 
agent has to place a given piece, and it has to choose 
where to place the piece on the board. Placing the 
piece is called an action, and since there are multiple 
ways to place the piece on the board, we have a range 
of possible of actions. In a given state the agent 
has to try each possible action, and the action 
with the highest evaluation, is the action the 
agent executes in the current state. 
When evaluating each possible action, the 
objective function S is used. As shown in the 
input-section, the objective function is 
defined as:

\begin{align*}
\valueFunction (\gameState ) &= \sum_{i=1}^{\dimensions} \weight _{i}\feature _{i}(\gameState )
\end{align*}

Where $F_i$ is the i'th feature function 
and w\_i is the i'th weight associated 
with the corresponding i'th feature function. 
More specifically w\_i is the value of 
the i'th dimension of the agent's vector.
A specific action then gets evaluated 
by computing S(x). The main factor of 
the evaluation value is how the action 
changed the board layout , since each 
feature function F\_i (x) looks at different 
"types" of board layouts (insert ref. to feature policy table).
