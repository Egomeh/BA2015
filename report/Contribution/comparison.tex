\subsection{Emperical comparison between CMA-ES and the Cross-entropy method}

Both the Cross-entropy method and CMA-ES are considered state-of-art
stochastic search algorithms, and both have been documented to perform very well
in various problems. The Cross-entropy method has proven 
suitable for solving various types of problems such as 
combinatorial optimization problems like 'matrix cut problem'
and the 'travelling salesman' \citep{cetut2014}. 
The Cross-entropy method is mentioned in the same paper, acknowledged for
its ability to solve problems of numerical nature. In the 
Cross-entropy method tutorial by Boer et al. they present 
an example of the Cross-entropy method applied to 
a K-means clustering in which it evidently performs very well.
Near the conclusion of the tutorial the authors suggest the 
Cross-entropy method for use in learning policies in Markov Decision 
Processes, which is exactly the field in with our experiments will apply the
algorithm.\\
\\
Much like the Cross-entropy method, the CMA-ES is indeed considered
a well performing algorithm for the type of problem we use for benchmarking.
The CMA-ES has been experimentally shown to be highly useful for optimizing multidimensional
problems which may have multiple optima, which is indeed also a reality when 
optimizing policies for Tetris \citep{hansen2004}.\\
\\
Both algorithms have been applied to Tetris in different settings, and they have
indeed both proven viable solutions when learning Tetris, at least through the
\textit{one-piece controller} method that we, and others previously, have used
for learning Tetris. Yet, at least when applied to the task of learning Tetris,
the two algorithms are, to our knowledge at least, never compared directly. Usually
the algorithms are used as a tool to tune the weighting of a feature set where the
main emphasis lies on the construction of the feature set. Our contribution to 
the research is therefore a comparison between the two algorithms
with heavy focus on the algorithms. Our goal is to experimentally investigate
whether one algorithm is favourable over the other in learning Tetris.
This will, regardless of the outcome, not decisively declare one algorithm 
to be better than the other, but hopefully gain some empirical clues on whether 
a task like optimizing Tetris calls for usage of one algorithm over the other.
When optimizing Tetris, the feature set is shown to have a paramount impact on
how good an agent can become. To make sure that one algorithm is not randomly performing
better when using a certain feature set, the experiments include a test with two different 
feature sets to ensure that we do not bias the conclusion towards performance with a 
single feature set. Furthermore, to truly get an unbiased comparison between the two 
algorithms, we conducted a series of experiments with the purpose of finding
the best setting for each of the algorithms. With these tests we were able to 
tell how to set up the final comparison experiment such that neither of the
algorithms suffered from poor configuration. The algorithms are both 
compared in their ability to find good solutions and in how long they will 
take to reach these. Both algorithms works iteratively, and they both tend to
reach some point of convergence from which they never explore significantly better 
solutions. In each iteration, the algorithms evaluates a set of solutions and one of the
performance measures is the number of evaluations the algorithms must make 
before they reach their final solutions. Therefore, we may also be able to conclude 
that one algorithm is able to find solutions after fewer evaluations.




