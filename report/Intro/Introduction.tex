\vspace*{0em}
\section{Introduction \label{sec:intro}}

This thesis will cover an experimental approach 
in comparing two state-of-the-art stochastic search
algorithms in the context of learning Tetris. 
Tetris is a classical game that has easy rules, but
is hard to master. In the game, the player is presented with
a board onto which differently shaped pieces fall. 
A horizontal line on the board is removed if all space between
the left and right edge of the board on this line are occupied with pieces.
The game is lost once the pile of pieces reach the top of the board,
and the task of the player is then to arrange the pieces such that
the pile stays as small as possible. 
The algorithms in consideration for learning Tetris
are the Cross-entropy method and 
the Covariance Matrix Adaptation Evolution Strategy (CMA-ES
for short). The algorithms are each characterized by their
ability to search in rough multidimensional 
search spaces that offers little possibility 
for analytical approaches.
Using a game such as Tetris for benchmarking machine learning
algorithms is quite common. The games have the appealing property 
that they are easy to interact with, usually have simple rules and 
goals but are very hard to play well. Games also often carry
a very decisive feedback on performance, often as a single number that 
indicates the score of the player. Thus games often present a system 
that is hard to approach analytically, but can easily be interacted with.
Most often, when machine learning
techniques are applied to problems such as games, conventional
learning methods fall short due to both  very large number of 
possible actions in the games and highly unpredictable mappings 
between actions and their long-term consequences. The individual playthroughs
in Tetris are considered episodic tasks, where some agent is placed
in the game environment and interacts with the game following some
strategy that will hopefully result in a high score. 
The agent will make decisions on how to react 
to the game while playing, and optimizing the agent is hence a task of
finding well performing policies for the agent to follow. Yet, as mentioned,
deciding what makes a policy good can be very difficult problems 
with large state spaces like in games. When applying reinforcement learning methods to learning
the policies, conventional full graph traversal approaches typically do not suffice 
due to infeasible computation times. We will cover how
Tetris can be formulated in way that allows  the Cross-entropy method 
and CMA-ES to search for policies, and we will experimentally 
attempt to obtain an empirical perception of whether of the two algorithms
are preferable in this scenario.

\input{Intro/rl}

\input{Intro/intro}

\input{Intro/goals}



