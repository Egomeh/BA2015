\section{Introduction \label{sec:intro}}

This thesis will cover an experimental approach 
by comparing two state-of-the-art stochastic optimization algorithms search methods for learning Tetris. 
The algorithms in consideration
are the Cross-entropy method and 
the Covariance Matrix Adaption Evolution Strategy (CMA-ES
for short). The algorithms are each characterized by their
ability to search in rough multidimensional 
search spaces that only offers little possibility 
for analytical approaches.
The algorithms are applied to the task of learning to play Tetris. 
Most often, when machine learning
techniques are applied to problems such as games, conventional
learning methods fall short due to both  very large number of 
possible actions in the games and highly unpredictable mappings 
between actions and their long-term consequences. The individual playthroughs
in Tetris are considered episodic tasks, where some agent is placed
in the game environment, and interacts with the game following some
strategy that will hopefully result in a high score. 
The agent will make decisions on how to react 
to the game while playing, and optimizing the agent is hence a task of
finding well performing policies for the agent to follow. Yet, as mentioned,
deciding what makes a policy good can be very difficult problems 
with large state spaces like games. When applying reinforcement methods to learning
the policies, conventional full graph traversal approaches typically do not suffice 
due to infeasible computation times. We will cover how
Tetris can be formulated in way that allows  the Cross-entropy method 
and CMA-ES to search for policies, and we will experimentally 
attempt to obtain an empirical perception of whether of the two algorithms
are preferable in this scenario.

\input{Intro/rl}

\input{Intro/intro}

\input{Intro/goals}

%\input{Intro/scope}


