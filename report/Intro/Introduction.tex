\section{Introduction \label{sec:intro}}

This thesis will cover an experimental approach 
to compare two optimization algorithms both considered 
state-of-the-art search methods. The algorithms will be applied to 
learning the game Tetris. Most often, when machine learning
techniques are applied to problems such as games, conventional
learning methods fall short due to both  very large number of 
possible actions in the games and to highly unpredictable mappings 
between actions and their long-term consequences. The games 
are considered and episodic task where some agent is placed
in the game environment and attempt to play the game as well
as possible. The agent will make decisions on how to react 
to the game while playing, and optimizing the agent is hence 
find as good as possible policies for the agent. Yet, as mentioned,
deciding what makes a policy good can be very difficult in
problems like games. Hence, the development of the policies 
is based on reinforcement learning where the only feedback 
on the policy is the score gained at the end of a game.

\input{Intro/rl}

\input{Intro/intro}

\input{Intro/goals}

\input{Intro/scope}


