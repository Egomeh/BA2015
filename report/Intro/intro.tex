\subsection{Learning Tetris \label{sec:learningTetris}}

A widely used benchmark
for reinforcement learning algorithms is designing agents 
to playing the classic video game of Tetris. Tetris is an 
appealing benchmarking problem due to it's complexity. The 
standard games plays on a board made from a grid that is
10 cells wide and 20 cells tall. As the game progress, differently
shaped pieces fall from the top of the board. 
When a row on the board is fully occupied by pieces, the line
is removed, all lines above it moved one line down and a score
point is given to the player. If a cell above the 20 first rows is
occupied, the game ends. The task of the player is to move
and rotate the falling pieces in a way that yields the highest 
score before the game ends.\\
\\
Tetris is indeed a hard task to computationally optimize, as
the game has a very high number of board configurations estimated to be
$10^{59}$ \citep{scherrer2009}. In relation to reinforcement learning,
an agent that plays Tetris is placed in an environment with a set of 
states far too large to exhaustively explore, and a set of transitions
that are stochastic. Because of this
complexity, a common approach 
in the literature is to use 
\textit{one-piece controllers}\footnote{Agents and controllers
both refer to artificial players.}, such as described by 
\cite{scherrer2009:b}. These controllers are only aware of
the current board state and the currently falling piece.
Hence, the policy of the agent is only to greedily choose
the action that transitions to the most rewarding state
from a single piece, which is typically less than 80 states\footnote{
Assuming 4 possible rotations and 20 columns
in which the piece may be dropped.}.
Using these controllers, the search space is reduced 
to only consider the current board, and the possible 
places to drop the piece. \\
The game used for the benchmarking is a simplified version of Tetris,
in which 
controllers need only to decide in what column to drop the current
piece, and what orientation the piece should have when dropped.
Thus, the simplified version of Tetris differs from the 
original game mainly in two significant aspects. 
First, the controller is 
disallowed to move the piece horizontally while the piece 
is falling. Second, the controller has 'infinite'
time to make its decision on where to drop the current piece.
This way, the game only progress in discrete time steps
when the controller takes
an action, whereas the classical game runs continuously 
regardless of the players actions.
Thus, the controller cannot take advantage of moving the piece 
during the fall, but is not restricted by the time limitations.
This is however a common way of benchmarking Tetris playing agents
\citep{scherrer2009}\\
\\
When the controller decides which action to take, it will
simulate each of the possible actions and choose the one that
leads to the most favourable board state. To evaluate the board 
state, the controller uses a set of features that defines 
various qualities of the board, and associate a weight to each 
feature. This means that the efficiency of the controller 
is determined by the features the controller is aware of
and how heavily they are weighted. This allows
the controller with $n$ features to be expressed as an 
$n$ dimensional real-valued vector $x \in \mathbb{R}^n$, with one dimension 
per feature, and the value in that dimension the weight.
A widely known controller from the literature is the Dellacherie's controller, 
as described in \cite{scherrer2009}. This controller
takes six features of the board into account, seen in table 
\ref{table:dellfeat} on page \pageref{table:dellfeat}. In relation 
to reinforcement learning, the policy of the agent is the feature set
combined with the weights of each feature. 
When using the \textit{one-piece controllers}, the policy remains 
the same during the episode, and the learning is based on updating
the policy from concluded games, rather than picking up on signals
that occur in the game as it plays.





