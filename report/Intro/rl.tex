\subsection{Reinforcement learning \label{RL}}

In the field of machine learning, the subfield of reinforcement learning
deals with training agents to behave in an environment through
trial and error. The environment typically consists of a set of states.
The agent can trough a set of actions transition between states.
The reinforcement learning methods relates somewhat 
to supervised and unsupervised learning models. In unsupervised 
learning, the agents are never given a feedback on their actions
and must attempt to derive some pattern without decisive responses.
In supervised leaning, the agent is trained from labelled data.
In this way, the agent will always, for any action, receive 
feedback of whether the committed action was desirable or not.\\
Reinforcement learning appears somewhat similar to supervised learning.
In reinforcement learning, the agent will commit actions in an environment
and receive reward based on those. The reward is a metric that the agent 
will attempt to accumulate
as much of as possible. In the game of Tetris, this is a direct mapping to 
the score achieved by the agent.
 The main difference between supervised
learning and reinforcement learning is that in reinforcement learning,
the agents cannot necessarily determine the correctness 
of a specific action as some actions, albeit yielding reward,
may cause relative loss of reward in the future. An example of how this applies 
to artificial agents is the game Tic Tac Toe. If supervised learning
is applied, the playing agent will for each move in the game know
if the move was considered good or bad. In reinforcement learning however,
the only feedback the agent will have is rewards that are given 
when reaching certain states of the game. 
Often in the case of games, the 
time frame naturally falls into playthroughs in which the 
goal of the agent is to accumulate as much reward as possible 
before the game ends. When the agent interacts 
with the environment, it transitions between different states through 
actions. \comment{When deciding which action to carry out, the agent will use
a policy that determines what actions to take depending on the circumstances.
As the games are non-deterministic, one can find expectations
of how much reward the agent will receive while following the policy.}
The goal is then to choose a policy, that when followed by the agent,
gives the highest possible cumulative reward at the end of a playthrough. 
It's important to note that the reward 
for an action may be negative, and will in such case actively discourage 
the agent for taking given actions.
The environment, in which the agent appears, often 
or at least for simplicity, has an exit state
in which the agent is terminated, and the cumulative reward 
is revealed. This is very much the case in games, where the goal 
is often to either reach the final state as fast as possible,
or to keep the game running for as long as possible.
The action transitions may 
not be deterministic and if they are not,
it's required that the probabilities of transitions
remain during the entire episode \citep{Carr}. In regards to Tetris, these probabilities of
transitions would refer to the frequency at which each piece occur, which must remain
unchanged throughout the entire playtrough.
The transition caused by an action not being deterministic
means that if an action is taken by the agent, the agent cannot 
with certainty predict which state it reaches.
If the state space 
is large enough, it becomes infeasible to attempt to compute 
the commutative reward when taking actions, as this would require
a thorough search of a far too large space. Thus, in situations 
with large state spaces, as Tetris will turn out to be, one must choose 
an approach for finding the best possible policies by other
means than exhaustive search. 
The methods used in reinforcement learning 
specify how the agent should change its policy according to 
results from earlier episodes, and the specific methods used in
the context of Tetris is explained in further detail in later sections.













