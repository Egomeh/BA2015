\section{CE (Cross Entropy)}
CE is described through many papers in 
slightly different ways. Using similar 
format as in the paper \citep{thiery:09}.\\
\\
The Cross-Entropy fall into the category of stochastic 
optimization algorithms
as those generally described in 
section \ref{Optimizers}. In particular, this 
method uses a Gaussian distribution and 
attempts to find distribution parameters 
that yields good candidates for the 
objective function $\fitnessFunction$.\\
\\
The Cross-Entropy method starts with an initial 
mean $\mean$ and variance $\varianceMatrix$. 
The mean is usually an $\dimensions$ dimensional vector
set to:
\begin{align*}
\mean = \begin{pmatrix}
0\\
\vdots\\
0
\end{pmatrix} 
\end{align*}

The variance is kept individual for each dimension, 
and is usually initialized as follows:

\begin{align*}
\varianceMatrix =
\begin{bmatrix}
\sigma_1 & \hdots & 0\\
\vdots & \ddots & \vdots\\
0 & \hdots & \sigma_{\dimensions}
\end{bmatrix}
\end{align*}

Where in this context, $\sigma$ represents \textbf{standard deviation}.\\
\\
The algorithm then works iteratively on generations of individual
search points acting as candidate inputs for the objective function.
In each generation, $\populationSize$ vectors are sampled by 
$x_{i} \sim \mathcal{N} \left(\mean, \varianceMatrix^{2} \right)
,\ i \in \{1,\dots,\populationSize \}$. The vectors are all evaluated 
against the fitness function and ordered such that $S \left( \individual_{1} \right) \geq, \dots, \geq S \left( \individual_{\populationSize} \right)$
, and the $\offspringNumber$ best are chosen for updating the distribution 
parameters. The mean is updated as the centroid of the chosen vectors, and
the variance is updated as the variance of the chosen vector in each 
dimension.\\

The pseudo code and details of the algorithm can be seen in the following.

\begin{figure}[H]
\hrule
\vspace{0.2cm}
{\centering  \textit{Noisy cross-entropy method}}
\vspace{0.2cm}
\hrule
\begin{algorithmic}
\State{\textbf{Input:}}
\State{\fitnessFunction : the function that estimates the performance of a vector $\individual$}
\State{($\mean$, $\varianceMatrix^2$): The mean and variance of the initial distribution}
\State{$\populationSize$ : The number of vectors sampled per generation/iteration}
\State{$\offspringNumber$: The number of offspring selected for the new mean}
\State{$\noise_{\generation}$: The noise added to each generation/iteration}
\\

\Loop
\State{Generate $\populationSize$ vectors $\individual_{1}, \individual_{2}, \dots, \individual_{\populationSize}$ from $\mathcal{N}(\mean, \varianceMatrix^2)$}
\State{Evaluate each vector using \fitnessFunction}
\State{Select the $\offspringNumber$ vectors with the highest evaluation}
\State{Update $\mean$ of the $\offspringNumber$ best vectors}
\State{Update $\varianceMatrix^2$ of the $\offspringNumber$ best vectors + $\noise_{\generation}$}
\EndLoop
\end{algorithmic}
\hrule
\end{figure}

\subsection{Input}

\textbf{The objective function} \\
As described in the 'Optimizers' section, CE is a general stochastic 
iterative algorithm that tries to solve an optimization problem of 
the form \citep{thiery:09}:
\begin{align*}
\hat{\textbf{\individual }} &= 
arg \  \underset{\textbf{\individual }}{max} \  
\fitnessFunction (\textbf{\individual }) \ 
\end{align*}
Where $x$ corresponds to a given vector, 
and S is our actual objective function. 
\\

\textbf{The mean and variance of the gaussian distribution} \\
Here $\mean$ is the mean and  $\varianceMatrix^2$ is the variance 
of the gaussian distribution ($\mean$,$\varianceMatrix^2$). 
More specifically this gaussian distribution is defined as 
\begin{align*}
\mathcal{N}(\mean,\varianceMatrix^2)
\end{align*}

\textbf{The number of vectors}\\
$\populationSize$ is the number of vectors sampled in each generation.
\\

\textbf{The number of offspring}\\
$\offspringNumber$ is the number of vectors which are used to compute 
the new mean, $\mean$, and variance,
$\varianceMatrix^2$, for next generation/iteration. 
These offspring vectors gets selected 
directly by taking $\offspringNumber$ vectors
which got the best evaluation.
\\

\textbf{The noise factor}\\
The noise factor, $\noise_{\generation}$, is the amount of noise which 
is applied to the variance $\varianceMatrix^2$ in iteration/generation 
$\generation$. In general, noise is used to avoid the risk of local optimum.
There are different kind of noise settings, such as: no noise, constant noise 
and linear decreasing noise \citep{szita:06}. When using no noise, $\noise_{\generation}$ 
is obviously set to zero. When using constant noise, the same value is 
added to the variance $\varianceMatrix^2$ in each iteration/generation. 
When using linear decreasing noise, $\noise_{\generation}$ is defined as, 
$\noise_{\generation}=max(5- \generation /10,0)$.
\\

\subsection{Loop}

\textbf{Sampling the population}\\
The first step of the loop is to create the new generation consisting of $\populationSize$ vectors. These vectors are created randomly within the distribution $\individual_{i}\sim \mathcal{N}(\mean,\varianceMatrix^2)$.
\\

\textbf{Evaluating the population}\\
After sampling the population, the algorithm needs to order the vectors to find the $\offspringNumber$ best vectors, each vector $\individual_{i}\ ,\ i \in \{1, \dots, \populationSize\}$ is evaluated using \fitnessFunction. The value from the fitness function then yields the estimated performance of each individual.
\\

\textbf{Selecting the offspring}\\
Now each $\individual_{i}$ has an assigned evaluation value, and the $\offspringNumber$ best vectors gets selected by taking the $\individual_{i}$ vectors with the highest evaluation value.
\\

\textbf{Updating the distribution parameters}\\
When updating the distribution parameters ($\mean$,$\varianceMatrix^2$), we first update
the mean $\mean$ by computing a total mean evaluation value of the $\offspringNumber$ best vectors. This is formally defined as:
\begin{align*}
\mean_{\generation +1}:=\frac{\sum_{i}^{\offspringNumber} \individual_{i}}{\offspringNumber}
\end{align*}
The variance $\varianceMatrix^2$ is updated to match the variance of $\offspringNumber$ best
values, such that the variance in dimension $i$ matches the variance of the $\offspringNumber$
in dimension $i$.
This is formally defined as:
\begin{align*}
\varianceMatrix^2_{\generation +1}:=\frac{\sum_{i}^{\offspringNumber}
(\individual_{i} - \mean_{\generation +1})^T(\individual_{i} - \mean_{\generation +1})}{\offspringNumber} + \noise_{\generation + 1}
\end{align*}
\\
