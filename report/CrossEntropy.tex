\section{CE (Cross Entropy) \label{CrossEntropy}}
CE is described through many papers in 
slightly different ways. Using similar 
format as in the paper \citep{thiery:09}.\\
\\
This method uses a Gaussian distribution and 
attempts to find distribution parameters 
that yields good candidates for the 
objective function $\fitnessFunction$.\\
\\
The Cross-Entropy method starts with an initial 
mean $\mean$ and standard deviation $\varianceMatrix$. 
The mean is usually an $\dimensions$ dimensional vector
set to:
\begin{align*}
\mean = \begin{pmatrix}
0\\
\vdots\\
0
\end{pmatrix} 
\end{align*}

The variance is kept individual for each dimension, 
and is usually initialized as follows:

\begin{align*}
\varianceMatrix =
\begin{bmatrix}
\sigma_1 & \hdots & 0\\
\vdots & \ddots & \vdots\\
0 & \hdots & \sigma_{\dimensions}
\end{bmatrix}
\end{align*}

Where in this context, $\sigma$ represents \textit{standard deviation}.\\
\\
The algorithm then works iteratively on generations of individual
search points acting as candidate inputs for the objective function.
In each generation, $\populationSize$ vectors are sampled by 
$\individual_{i} \sim \mathcal{N} \left(\mean, \varianceMatrix^{2} \right)
,\ i \in \{1,\dots,\populationSize \}$. The vectors are all evaluated 
against the fitness function and ordered such that $\fitnessFunction \left( \individual_{1} \right) \geq, \dots, \geq \fitnessFunction \left( \individual_{\populationSize} \right)$
, and the $\offspringNumber$ best are chosen for updating the distribution 
parameters. The mean is updated as the centroid of the chosen vectors, and
the variance is updated as the variance of the chosen vector in each 
dimension.\\

The pseudo code and details of the algorithm can be seen in figure
\ref{fig:ceCode} on page \pageref{fig:ceCode}.

\begin{figure}[H]
\hrule
\vspace{0.2cm}
{\centering  \textit{Noisy cross-entropy method}}
\vspace{0.2cm}
\hrule
\begin{algorithmic}
\State{\textbf{input}}
\State{$\fitnessFunction$ : The function that estimates the performance of a vector $\individual$}
\State{($\mean_0$, $\varianceMatrix^2_0$): The mean and variance of the initial distribution}
\State{$\populationSize$ : The number of vectors sampled per generation/iteration}
\State{$\offspringNumber$: The number of offspring selected for the new mean}
\State{$\noise_{\generation}$: The noise added to each generation/iteration}
\\

\Loop
\State{Generate $\populationSize$ vectors $\individual_{1}, \individual_{2}, \dots, \individual_{\populationSize}$ from $\mathcal{N}(\mean_{\generation}, \varianceMatrix^2_{\generation})$}
\State{Evaluate each vector using $\fitnessFunction$}
\State{Select the $\offspringNumber$ vectors with the highest evaluation}
\State{Update $\mean_{\generation + 1}$ of the $\offspringNumber$ best vectors}
\State{Update $\varianceMatrix^2_{\generation + 1}$ of the $\offspringNumber$ best vectors + $\noise_{\generation}$}
\EndLoop
\end{algorithmic}
\hrule
\caption{The pseudo code for the Cross-Entropy algorithm \label{fig:ceCode}}
\end{figure}

\subsection{Input}

\textbf{The objective function \label{CEObjective}} \\
The function used to assess the value of a sampled vector.
As described in the 'Optimizers' section, CE is a general stochastic 
iterative algorithm that tries to solve an optimization problem of 
the form \citep{thiery:09}:
\begin{align*}
\hat{\textbf{\individual }} &= 
arg \  \underset{\textbf{\individual }}{max} \  
\fitnessFunction (\textbf{\individual }) \ 
\end{align*}
Where $x$ corresponds to a given vector, 
and $\fitnessFunction$ is our actual objective function. 
\\

\textbf{The mean and variance of the gaussian distribution} \\
Here $\mean_{\generation}$ is the mean and  
$\varianceMatrix^2_{\generation}$ is the variance 
of the gaussian distribution ($\mean_{\generation}$,
$\varianceMatrix^2_{\generation}$). 
More specifically this gaussian distribution is defined as 
\begin{align*}
\mathcal{N}(\mean_{\generation},\varianceMatrix^2_{\generation})
\end{align*}

Where $\generation$ denotes the current iteration.\\


\textbf{The number of vectors}\\
$\populationSize$ is the number of vectors sampled in each generation.
\\

\textbf{The number of offspring}\\
$\offspringNumber$ is the number of vectors which are used to compute 
the new mean, $\mean_{\generation + 1}$, and variance,
$\varianceMatrix^2_{\generation + 1}$, for next generation/iteration. 
These offspring vectors gets selected 
directly by taking $\offspringNumber$ vectors
which got the best evaluation.
\\

\textbf{The noise factor}\\
The noise factor, $\noise_{\generation}$, is the amount of noise which 
is applied to the variance $\varianceMatrix^2$ in iteration/generation 
$\generation$. In general, noise is used to avoid the risk of a local optimum.
There are different kinds of noise settings, such as: no noise, constant noise 
and linear decreasing noise \citep{szita:06}. 
When using no noise, $\noise_{\generation}$ 
is simply set to zero. When using constant noise, the same value is 
added to the variance $\varianceMatrix^2$ in each iteration/generation. 
When using linear decreasing noise, $\noise_{\generation}$ is defined as
$\noise_{\generation}=max(5- \generation /10,0)$.
\\

\subsection{Loop}

\textbf{Sampling the population}\\
The first step of the loop is to create the new generation consisting of $\populationSize$ vectors. These vectors are sampled randomly within the distribution $\individual_{i}\sim \mathcal{N}(\mean_{\generation},\varianceMatrix^2_{\generation})$.
\\

\textbf{Evaluating the population}\\
After sampling the population, the algorithm needs to order the vectors to find the $\offspringNumber$ best vectors, each vector $\individual_{i}\ ,\ i \in \{1, \dots, \populationSize\}$ is evaluated using $\fitnessFunction$. 
The value from the objective function then yields 
the estimated performance of each individual.
\\

\textbf{Selecting the offspring}\\
As each $\individual_{i}$ has an assigned evaluation value, and the $\offspringNumber$ best vectors gets selected by taking the $\individual_{i}$ vectors with the highest evaluation value.
\\

\textbf{Updating the distribution parameters}\\
When updating the distribution parameters for the next iteration
($\mean_{\generation + 1}$,$\varianceMatrix^2_{\generation + 1}$), 
the mean is updated by computing the centroid of the 
$\offspringNumber$ best vectors. This is formally defined as:
\begin{align*}
\mean_{\generation +1}:=\frac{\sum_{i}^{\offspringNumber} \individual_{i}}{\offspringNumber}
\end{align*}
The variance $\varianceMatrix^2_{\generation + 1}$ is updated 
to match the variance of the $\offspringNumber$ best
vectors, such that the variance in dimension $i$ 
matches the variance of the $\offspringNumber$
in dimension $i$.
This is formally defined as:
\begin{align*}
\varianceMatrix^2_{\generation +1}:=\frac{\sum_{i}^{\offspringNumber}
(\individual_{i} - \mean_{\generation +1})^T(\individual_{i} - \mean_{\generation +1})}{\offspringNumber} + \noise_{\generation + 1}
\end{align*}
\\
