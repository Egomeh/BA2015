\section{CMA-ES}


This description is based on the tutorial written by Nikolaus Hansen
\citep{hansen2011}. The section will not in detail cover how 
the CMA-ES is derived, but rather how it deviates from Cross Entropy.\\
\\
The CMA-ES operates on a general level much like the Cross Entropy 
method, but includes some features that increases the adaptability 
of the algorithm.\\
\\
The CMA-ES, like the Cross Entropy, uses a Gaussian distribution to
search for good solutions to $\fitnessFunction$. Yet, in the second argument 
in the Gaussian distribution, the CMA-ES provides a covariance matrix.
As the Cross Entropy method only provides a diagonal matrix of scalers
it's restricted to only scaling the ellipsoid of equal density along
the coordinate axes. The CMA-ES however, with a full covariance matrix,
allows the ellipsoid to rotate arbitrarily in the search space.\\
\\
While the Cross Entropy only considers the next population when updating the
distribution parameters, the CMA-ES keeps some information from earlier 
generations. This allows the CMA-ES somewhat keep track of the evolution 
of the sampled vectors.\\
\\
The CMA-ES also differs from Cross Entropy in how it evaluates the influence 
of the offspring vectors. As Cross Entropy weights all vectors equally when 
moving the mean. The CMA-ES, at least from the implementation in SHARK, 
has the option of taking a weighted combination of the offspring in order
to bias towards the better vectors.

\begin{figure}[H]
\hrule
\vspace{0.2cm}
{\centering  \textit{CMA-ES}}
\vspace{0.2cm}
\hrule
\begin{algorithmic}
\State{\textbf{input}}
\State{$\fitnessFunction$ : The function that estimates the performance of a vector $\individual$}
\State{($\mean$, $C$): The mean and variance of the initial distribution, where 
$C$ is the covariance matrix usually set to $C=I$}
\State{$\populationSize$ : The number of vectors sampled per generation/iteration}
\State{$\offspringNumber$: The number of offspring selected for the new mean}
\\
\State{\textbf{initialization}}
\State{Set initial internal parameters}
\\
\Loop
\State{Sample new generation}
\State{Evaluate each vector using $\fitnessFunction$ and recombine}
\State{Step-size control}
\State{Covariance matrix adaption}
\EndLoop
\end{algorithmic}
\hrule
\caption{The pseudo code for the Cross-Entropy algorithm \label{fig:ceCode}}
\end{figure}

\comment{All CMA specific beow this is NOT DONE!}

\subsection{Input}

\textbf{The objective function} \\
This serves the same purpose as in Cross Entropy (see page \pageref{CEObjective}).
\\

\textbf{The mean and variance of the gaussian distribution (Adapt to CMA)} \\
Here $\mean_{\generation}$ is the mean and  
$C^{\generation}$ is the variance 
of the gaussian distribution ($\mean_{\generation}$,
$\varianceMatrix^2_{\generation}$). 
More specifically this gaussian distribution is defined as 
\begin{align*}
\mathcal{N}(\mean_{\generation},\varianceMatrix^2_{\generation})
\end{align*}

Where $\generation$ denotes the current iteration.\\


\textbf{The number of vectors}\\
$\populationSize$ is the number of vectors sampled in each generation.
\\

\textbf{The number of offspring}\\
$\offspringNumber$ is the number of vectors which are used to compute 
the new mean, $\mean_{\generation + 1}$, and variance,
$\varianceMatrix^2_{\generation + 1}$, for next generation/iteration. 
These offspring vectors gets selected 
directly by taking $\offspringNumber$ vectors
which got the best evaluation.
\\


\subsection{Initialization}


Set parameters
\begin{align*}
\populationSize, \offspringNumber, w_{i \dots \offspringNumber}, c_{\sigma}, d_{\sigma}, c_c, c_1, c_{\offspringNumber}
\end{align*}
To their default values according to table 1 in \citep{hansen2011}.

Set evolution path $p_{\sigma} = 0$, $p_{c} = 0$, covariance matrix $C = I$ and $\generation = 0$

\subsection{Loop}

\textbf{Sample new generation}\\
Sample new population of search points, for $k = 1, \dots, \populationSize$

\begin{align*}
z_{k} &\sim \mathcal{N}(0, I)\\
y_{k} &= BDz_{k} \sim \mathcal{N}(o, C)\\
x_{k} &= m + \sigma y_{k} \sim \mathcal{N}(m, \sigma^2 C)
\end{align*}

\textbf{Evaluate each vector using $\fitnessFunction$ and recombine}\\
Selection and recombination

\begin{align*}
\langle y \rangle_{w} &= \sum^{\offspringNumber}_{i = 1}w_{i} y_{i : \populationSize}, \text{where} \sum^{\offspringNumber}_{i = 1} w_{i} = 1, w_{i} > 0\\
m &\leftarrow m + \sigma \langle y \rangle_{w} = \sum^{\offspringNumber}_{i = 1} w_{i}x_{i:\populationSize}
\end{align*}

\textbf{Step-size control}

\begin{align*}
p_{\sigma} &\leftarrow \left( 1 - c_{\sigma} \right) p_{\sigma} + \sqrt{c_{\sigma} \left( 2 - c_{\sigma} \right) \offspringNumber_{\text{eff}}} C^{- \frac{1}{2}} \langle y \rangle_{w}\\
\sigma &\leftarrow \sigma \times exp \left( \frac{c_{\sigma}}{d_{\sigma}} \left( \frac{||p_{\sigma}||}{E|| \mathcal{N}(o, I) ||} - 1 \right) \right)
\end{align*}

\textbf{Covariance matrix adaption}

\begin{align*}
p_{c} &\leftarrow \left( 1 - c_{c} \right) p_{c} + h_{\sigma} \sqrt{c_{\sigma} \left( 2 - c_{c} \right) \offspringNumber_{\text{eff}}} \langle y \rangle_{w}\\
C &\leftarrow \left( 1 - c_{c} - c_{\offspringNumber} \right) C + c_{1} \left( p_{c} p_{c}^{T} + \delta \left( h_{\sigma} \right) C \right) + c_{\offspringNumber} \sum^{\offspringNumber}_{i = 1} w_{i} y_{i : \offspringNumber} y_{i : \offspringNumber} ^{T}
\end{align*}







