\section{Introduction \label{sec:intro}}

On the topic of reinforcement learning, a widely used benchmark
for learning algorithms are designing agents 
for playing the classical game of Tetris. Tetris is an 
appealing benchmarking problem due to it's complexity. The 
standard games plays on a board made from a grid that is
10 cells wide and 20 cells high. As the game progress, differently
shaped pieces fall from the top of the board. 
When a row on the board is fully occupied by pieces, the line
is removed, all lines above it moved one line down and a score
point is given to the player. If a cell above the 20 rows first is
occupied, the game ends. The task of the player is to move
and rotate the falling pieces in a way that yields the highest 
score before the game ends.\\
\\
Tetris is indeed a hard task to computationally optimize, as
the game has a very high number of board configurations estimated to be
$10^{59}$ \citep{scherrer2009}. Because of this
complexity, a common approach 
in the literature is to use 
\textit{one-piece controllers}\footnote{Agents and controllers
both refer to artificial players.}, such as described in 
\cite{scherrer2009:b}. These controllers are aware of only 
the current board state and the currently falling piece.
Using these controllers, the search space is reduced 
to only looking at the current board, and the possible 
places to drop the piece. The 
game used for the benchmark is a simplified version of Tetris,
in which 
controllers need only to decide in what column to drop the current
piece, and what orientation the piece should have when dropped.
Thus, the simplified version of Tetris differs from the 
original game in two main ways. One is that the controller is 
disallowed to move the piece horizontally while the piece 
is falling. The other is that the controller has 'infinite'
time to make its decision on where to drop the current piece.
Thus, the controller cannot take advantage of moving the piece 
during the fall, but is not restricted by the time limitations.
This is however a common way of benchmarking Tetris controllers 
\citep{scherrer2009}\\
\\
When the controllers decide which action to take, it will
simulate each of the possible actions and choose the one that
leads to the most favourable board state. To evaluate the board 
state, the controller uses a set of features that defines 
various qualities of the board, and associate a weight to each 
feature. This means that the efficiency of the controller 
is determined by which features of the board are considered by
and how heavily they are weighted. This allows
the controller with $n$ features to be expressed as an 
$n$ dimensional real-valued vector, with one dimension 
per feature, and the value in that dimension the weight.
An often referred to controller is the Dellacherie's controller, 
as described in \cite{scherrer2009}. This controller
takes six features of the board into account, seen in table 
\ref{table:dellfeat}.



\subsection{Goals of the thesis}

Both the Cross-Entropy and the CMA-ES methods has been used 
in learning Tetris with \textit{one piece controllers}, but as 
mentioned, to our knowledge, only little effort has been put into 
comparing the two methods. This thesis will explore
how the two methods compare against each other under similar
conditions. Therefore, we will use a set of features among those
commonly used, and compare how the two optimizing algorithms 
differ. The goal however is not to find a controller that 
outperforms existing controllers, but only to investigate 
how the Cross-Entropy and CMA-ES differs when learning Tetris
with similar controllers.\\
\\
In this paper, we will investigate the state-of-the-art Tetris and also the optimization 
algorithms, CMA-ES and CE, to get a theoretical understanding of the task and how they 
apply to Tetris.\\
From the the SHARK library \citep{shark08} we have a working implementation of the CMA-ES 
algorithm. However, the Cross-Entropy method is not present in the SHARK library and thus 
we must implement it ourselves according to the other researchers work. To document the 
soundness of the implemented CE method, we will replicate other researchers experiments and 
verify that we obtain the same results.\\
Then we will benchmark CMA-ES and CE against eachother 
to determine if one yields better optimization results than the other.
