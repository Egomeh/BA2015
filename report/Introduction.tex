\section{Introduction \label{sec:intro}}

On the topic of reinforcement learning, a widely used benchmark
for learning algorithms are designing agents 
for playing the classical game of Tetris. Tetris is an 
appealing benchmarking problem due to it's complexity. The 
standard games plays on a board made from a grid that is
10 cells wide and 20 cells heigh. As the game plays, differently
shaped pieces fall from the top of the board. 
When a row on the board is fully occupied by pieces, the line
is removed, all lines above it moved one line down and a score
point is given to the player. If a cell above the 20 rows first is
occupied, the game ends. The task of the player is to move
and rotate the falling pieces in a way that yields the highest 
score before the game ends.\\
\\
This game is indeed a hard task to computationally optimize, as
the game has a very high number of board configurations estimated to be
$10^{59}$ (\cite{scherrer2009}). Bacause of this
complexity, a common approach 
in the literature is to use 
\textit{one-piece controllers}\footnote{Agents and controllers
both refer to artificial players.}, such as described in 
\cite{scherrer2009:b}. These controllers are aware of only 
the current board state and the currently falling piece.
Using these controllers, the search space is reduced 
to only looking at the current board, and the possible 
places to drop the piece. The 
game used for the banchmark is a simplified version of Tetris,
in which 
controllers need only to decide in what column to drop the current
piece, and what orientation the piece should have when dropped.
Thus, the simplified version of Tetris differs from the 
original game in two main ways. One is that the controller is 
disallowed to move the piece horizontally while the piece 
is falling. The other is that the controller has 'infinite'
time to make its decision on where to drop the current piece.
Thus, the controller cannot take advantage of moving the piece 
during the fall, but is not restricted by the time limitations.
This is however a common way of benchmarking Tetris controllers 
(\citep{scherrer2009})\\
\\
When the controllers decide which action to take, it will
simulate each of the possible actions and choose the one that
leads to the most favourable board state. To evaluate the board 
state, the controller uses a set of features that defines 
various qualities of the board, and associate a weight to each 
feature. This means that the efficiency of the controller 
is determined by which features of the board are considered by
and how heavily they are weighted. This allows
the controller with $n$ features to be expressed as an 
$n$ dimensional real-valued vector, with one dimension 
per feature, and the value in that dimension the weight.
An often referred to controller is the Dellacherie's controller,
described in as described in \cite{scherrer2009}. This controller
takes six features of the board into account, seen in table 
\ref{table:dellfeat}.

\begin{figure}[h!]
\begin{center}
\begin{tabular}{| l | p{8cm} |}
\hline
\textbf{Feature} & \textbf{Description}\\
\hline
Landing height & The height of the piece when it lands\\
\hline
Eroded piece cells & Number of rows cleared in the last move
times the number of bricks cleared from the last move\\
\hline
Row transitions & Number of horizontal cell transitions\\
\hline
Column transitions & Number of vertical cell transitions \\
\hline
Holes & Number of empty cells covered by a full cell\\
\hline
Board wells & cumulative sum of cells to the depth of
the board wells.\\
\hline
\end{tabular}
\end{center}
\caption{features of the Dellacherie controller \label{table:dellfeat}}
\end{figure}


\subsection{Goals of the thesis}


Both the Cross-Entropy and the CMA-ES methods has been used 
in learning Tetris with \textit{one piece controllers}, but as 
mentioned, to our knowledge, only little effort has been put into 
comparing the two methods. This thesis will explore
how the two methods compare against each other under similar
conditions. Therefore, we will use a set of features among those
commonly used, and compare how the two optimizing algorithms 
differ. The goal however is not to find a controller that 
outperforms existing controllers, but only to investigate 
how the Cross-Entropy and CMA-ES differs when learning Tetris
with similar controllers.

