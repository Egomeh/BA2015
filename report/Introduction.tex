\section{Introduction \label{sec:intro}}

On the topic of reinforcement learning, a widely used benchmark
for learning algorithms are designing agents 
for playing the classical game of Tetris. Tetris is an 
appealing benchmarking problem due to it's complexity. The 
standard games plays on a board made from a grid that is
10 cells wide and 20 cells heigh. As the game plays, differently
shaped pieces fall from the top of the board. 
When a row on the board is fully occupied by pieces, the line
is removed, all lines above it moved one line down and a score
point is given to the player. If a cell above the 20 rows first is
occupied, the game ends. The task of the player is to move
and rotate the falling pieces in a way that yields the highest 
score before the game ends.\\
\\
This game is indeed a hard task to computationally optimize, as
the game has a very high number of board configurations, 
$10^{59}$ estimated in \cite{scherrer2009}. Bacause of this
complexity, a common approach 
in the literature is to use 
\textit{one-piece controllers}\footnote{Agents and controllers
both refer to artificial players.}, such as described in 
\cite{scherrer2009:b}. These controllers are aware of only 
the current board state and the currently falling piece.
Using these controllers, the search space is reduced 
to only looking at the current board, and the possible 
places to drop the piece. The 
game used for the banchmark is a simplified version of Tetris,
in which 
controllers need only to decide in what column to drop the current
piece, and what orientation the piece should have when dropped.
This disallows the controller to move the piece while 
the piece is falling, however, in the literature, this 
is not regarded a significant limitation.\\
\\
When the controllers decide which action to take, it will
simulate each of the possible actions and choose the one that
leads to the most favourable board state. To evaluate the board 
state, the controller uses a set of features that defines 
various qualities of the board, and associate a weight to each 
feature. This means that the efficiency of the controller 
is determined by which features of the board are considered by
and how heavily they are weighted. This allows
the controller with $n$ features to be expressed as an 
$n$ dimensional real-valued vector, with one dimension 
per feature, and the value in that dimension the weight.
An often referred to controller is the Dellacherie's controller,
described in as described in \cite{scherrer2009}. This controller
takes six features of the board into account, seen in table 
\ref{table:dellfeat}.

\begin{figure}[h!]
\begin{center}
\begin{tabular}{| l | p{8cm} |}
\hline
\textbf{Feature} & \textbf{Description}\\
\hline
Landing height & The height of the piece when it lands\\
\hline
Eroded piece cells & Number of rows cleared in the last move
times the number of bricks cleared from the last move\\
\hline
Row transitions & Number of horizontal cell transitions\\
\hline
Column transitions & Number of vertical cell transitions \\
\hline
Holes & Number of empty cells covered by a full cell\\
\hline
Board wells & cumulative sum of cells to the depth of
the board wells.\\
\hline
\end{tabular}
\end{center}
\caption{features of the Dellacherie controller \label{table:dellfeat}}
\end{figure}


\subsection{Previous work}

Over the time, many researchers has tried various feature 
sets and applied various optimizers to find the best 
possible Tetris controllers. The features used are typically
ones that attempt to mimic the board conditions that would
normally catch the attention of a human player, such as
how high the overall pile of pieces is and how many holes 
the board has. In \cite{scherrer2009:b} table 1, a table 
presents some feature sets used throughout various publications
on the subject. In later works, using evolutionary stochastic 
search methods for tuning the weights of the feature sets towards
efficient controllers. A widely used method is the 
Cross-Entropy method described in detail in \citep{cetut2014}. 
Especially a noisy version has proven to be quite effective in the
domain of learning of learning Tetris as 
described in \cite{szita:06}.\\
\\
Another optimization
method introduced to the domain  is the Covariance Matrix
Adaption Evolution Strategy (CMA-ES) described in detail in 
\cite{hansen2011}. This method is used in learning Tetris in 
\cite{boumaza2011:a}. The Cross-Entropy and the CMA-ES are
both considered state-of-the-art algorithms for this kind 
of problem, but to our knowledge, not many attempts to 
compare the two directly has been made.

\subsection{Goals of the thesis}


Both the Cross-Entropy and the CMA-ES methods has been used 
in learning Tetris with \textit{one piece controllers}, but as 
mentioned, to our knowledge, only little effort has been put into 
comparing the two methods. This thesis will explore
how the two methods compare against each other under similar
conditions. Therefore, we will use a set of features among those
commonly used, and compare how the two optimizing algorithms 
differ. The goal however is not to find a controller that 
outperforms existing controllers, but only to investigate 
how the Cross-Entropy and CMA-ES differs when learning Tetris
with similar controllers.


\subsection{Scope\label{section:scope}}

The experiments will be carried out on the simplified version of
Tetris using the MDPTetris software found at \cite{mdptetris},
which is the same emulator used in \cite{scherrer2009:b}.
This software already have the well known feature sets
implemented, so we will not ourselves extend any of the features.
For comparing the optimizers, the Shark\footnote{See \url{http://image.diku.dk/shark/} or  \cite{shark08}
} library will be used. This library already contains an
implementation of the CMA-ES optimizer, but lack the 
Cross-Entropy. Therefore, a part of this thesis will
be to implement and document the Cross-Entropy method in Shark.


\subsection{Limitations}

What we will not do, and what limitations and constrains
we figure out during the project.

